#!/usr/bin/env python3
"""
å®éªŒç»“æœåˆ†æè„šæœ¬
ç”¨äºåˆ†æå®Œæ•´çš„60ä¸ªepochè®­ç»ƒç»“æœ

ä½œè€…ä¿®æ”¹ï¼šåˆ›å»ºå®éªŒç»“æœåˆ†æè„šæœ¬
åŠŸèƒ½ï¼šåˆ†æåŸºçº¿å®éªŒå’Œåˆ›æ–°ç‚¹å®éªŒçš„æœ€ç»ˆæ€§èƒ½æŒ‡æ ‡
æ’¤é”€æ–¹æ³•ï¼šåˆ é™¤æ­¤æ–‡ä»¶
"""

import os
import sys
import re
import json
import argparse
from datetime import datetime

def extract_final_metrics(log_file_path):
    """
    ä»è®­ç»ƒæ—¥å¿—ä¸­æå–æœ€ç»ˆæ€§èƒ½æŒ‡æ ‡
    
    ä½œè€…ä¿®æ”¹ï¼šåˆ›å»ºæ€§èƒ½æŒ‡æ ‡æå–å‡½æ•°
    åŠŸèƒ½ï¼šä»è®­ç»ƒæ—¥å¿—ä¸­æå–mAPã€Rank-1ã€Rank-5ç­‰æœ€ç»ˆæŒ‡æ ‡
    æ’¤é”€æ–¹æ³•ï¼šåˆ é™¤æ­¤å‡½æ•°
    """
    if not os.path.exists(log_file_path):
        print(f"âš ï¸  æ—¥å¿—æ–‡ä»¶ä¸å­˜åœ¨: {log_file_path}")
        return None
    
    metrics = {
        'final_loss': 0,
        'final_accuracy': 0,
        'best_accuracy': 0,
        'mAP': 0,
        'Rank-1': 0,
        'Rank-5': 0,
        'total_epochs': 0,
        'training_time': 0
    }
    
    try:
        with open(log_file_path, 'r', encoding='utf-8') as f:
            content = f.read()
        
        # æå–æœ€ç»ˆæŸå¤±å’Œå‡†ç¡®ç‡
        loss_pattern = r'Loss: ([\d.]+)'
        acc_pattern = r'Acc: ([\d.]+)'
        
        losses = re.findall(loss_pattern, content)
        accuracies = re.findall(acc_pattern, content)
        
        if losses:
            metrics['final_loss'] = float(losses[-1])
        if accuracies:
            metrics['final_accuracy'] = float(accuracies[-1])
            metrics['best_accuracy'] = max([float(acc) for acc in accuracies])
        
        # æå–epochä¿¡æ¯
        epoch_pattern = r'Epoch\[(\d+)\]'
        epochs = re.findall(epoch_pattern, content)
        if epochs:
            metrics['total_epochs'] = max([int(epoch) for epoch in epochs])
        
        # æå–mAPå’ŒRankæŒ‡æ ‡ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        map_pattern = r'mAP: ([\d.]+)'
        rank1_pattern = r'Rank-1: ([\d.]+)'
        rank5_pattern = r'Rank-5: ([\d.]+)'
        
        map_matches = re.findall(map_pattern, content)
        rank1_matches = re.findall(rank1_pattern, content)
        rank5_matches = re.findall(rank5_pattern, content)
        
        if map_matches:
            metrics['mAP'] = float(map_matches[-1])
        if rank1_matches:
            metrics['Rank-1'] = float(rank1_matches[-1])
        if rank5_matches:
            metrics['Rank-5'] = float(rank5_matches[-1])
        
        return metrics
        
    except Exception as e:
        print(f"âŒ åˆ†ææ—¥å¿—æ–‡ä»¶æ—¶å‡ºé”™: {str(e)}")
        return None

def generate_comparison_report(baseline_metrics, moe_metrics, output_dir):
    """
    ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    
    ä½œè€…ä¿®æ”¹ï¼šåˆ›å»ºå®éªŒç»“æœå¯¹æ¯”æŠ¥å‘Šç”Ÿæˆå‡½æ•°
    åŠŸèƒ½ï¼šå¯¹æ¯”åŸºçº¿å®éªŒå’Œåˆ›æ–°ç‚¹å®éªŒçš„æœ€ç»ˆç»“æœ
    æ’¤é”€æ–¹æ³•ï¼šåˆ é™¤æ­¤å‡½æ•°
    """
    print("\nğŸ“Š ç”Ÿæˆæœ€ç»ˆå¯¹æ¯”æŠ¥å‘Š...")
    
    if not baseline_metrics or not moe_metrics:
        print("âŒ æ— æ³•ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Šï¼šç¼ºå°‘å¿…è¦çš„æŒ‡æ ‡æ•°æ®")
        return
    
    # è®¡ç®—æå‡å¹…åº¦
    improvements = {}
    for key in ['final_accuracy', 'best_accuracy', 'mAP', 'Rank-1', 'Rank-5']:
        if baseline_metrics[key] > 0 and moe_metrics[key] > 0:
            improvement = moe_metrics[key] - baseline_metrics[key]
            improvement_percent = (improvement / baseline_metrics[key]) * 100
            improvements[key] = {
                'absolute': improvement,
                'percentage': improvement_percent
            }
    
    # ç”ŸæˆæŠ¥å‘Š
    report = {
        "experiment_summary": {
            "baseline": baseline_metrics,
            "innovation": moe_metrics
        },
        "performance_comparison": improvements,
        "conclusion": {}
    }
    
    # ç”Ÿæˆç»“è®º
    if improvements.get('final_accuracy', {}).get('absolute', 0) > 0:
        report["conclusion"]["accuracy"] = "âœ… åˆ›æ–°ç‚¹åœ¨æœ€ç»ˆå‡†ç¡®ç‡ä¸Šæœ‰æå‡"
    else:
        report["conclusion"]["accuracy"] = "âŒ åˆ›æ–°ç‚¹åœ¨æœ€ç»ˆå‡†ç¡®ç‡ä¸Šæ— æå‡"
    
    if improvements.get('mAP', {}).get('absolute', 0) > 0:
        report["conclusion"]["mAP"] = "âœ… åˆ›æ–°ç‚¹åœ¨mAPæŒ‡æ ‡ä¸Šæœ‰æå‡"
    else:
        report["conclusion"]["mAP"] = "âŒ åˆ›æ–°ç‚¹åœ¨mAPæŒ‡æ ‡ä¸Šæ— æå‡"
    
    # ä¿å­˜æŠ¥å‘Š
    report_file = os.path.join(output_dir, "final_comparison_report.json")
    with open(report_file, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    # æ‰“å°æŠ¥å‘Š
    print("\n" + "="*60)
    print("ğŸ“Š æœ€ç»ˆå®éªŒç»“æœå¯¹æ¯”æŠ¥å‘Š")
    print("="*60)
    
    print(f"\nğŸ”¬ åŸºçº¿å®éªŒç»“æœ:")
    print(f"   æœ€ç»ˆæŸå¤±: {baseline_metrics['final_loss']:.4f}")
    print(f"   æœ€ç»ˆå‡†ç¡®ç‡: {baseline_metrics['final_accuracy']:.4f} ({baseline_metrics['final_accuracy']*100:.2f}%)")
    print(f"   æœ€ä½³å‡†ç¡®ç‡: {baseline_metrics['best_accuracy']:.4f} ({baseline_metrics['best_accuracy']*100:.2f}%)")
    if baseline_metrics['mAP'] > 0:
        print(f"   mAP: {baseline_metrics['mAP']:.2f}%")
    if baseline_metrics['Rank-1'] > 0:
        print(f"   Rank-1: {baseline_metrics['Rank-1']:.2f}%")
    if baseline_metrics['Rank-5'] > 0:
        print(f"   Rank-5: {baseline_metrics['Rank-5']:.2f}%")
    
    print(f"\nğŸš€ åˆ›æ–°ç‚¹å®éªŒç»“æœ:")
    print(f"   æœ€ç»ˆæŸå¤±: {moe_metrics['final_loss']:.4f}")
    print(f"   æœ€ç»ˆå‡†ç¡®ç‡: {moe_metrics['final_accuracy']:.4f} ({moe_metrics['final_accuracy']*100:.2f}%)")
    print(f"   æœ€ä½³å‡†ç¡®ç‡: {moe_metrics['best_accuracy']:.4f} ({moe_metrics['best_accuracy']*100:.2f}%)")
    if moe_metrics['mAP'] > 0:
        print(f"   mAP: {moe_metrics['mAP']:.2f}%")
    if moe_metrics['Rank-1'] > 0:
        print(f"   Rank-1: {moe_metrics['Rank-1']:.2f}%")
    if moe_metrics['Rank-5'] > 0:
        print(f"   Rank-5: {moe_metrics['Rank-5']:.2f}%")
    
    print(f"\nğŸ“ˆ æ€§èƒ½æå‡å¯¹æ¯”:")
    for key, improvement in improvements.items():
        if improvement['absolute'] > 0:
            print(f"   {key}: +{improvement['absolute']:.4f} (+{improvement['percentage']:.2f}%)")
        else:
            print(f"   {key}: {improvement['absolute']:.4f} ({improvement['percentage']:.2f}%)")
    
    print(f"\nğŸ¯ ç»“è®º:")
    for key, conclusion in report["conclusion"].items():
        print(f"   {conclusion}")
    
    print(f"\nğŸ“ è¯¦ç»†æŠ¥å‘Šå·²ä¿å­˜åˆ°: {report_file}")

def main():
    """
    ä¸»å‡½æ•°ï¼šåˆ†æå®éªŒç»“æœ
    
    ä½œè€…ä¿®æ”¹ï¼šåˆ›å»ºç»“æœåˆ†æä¸»æµç¨‹
    åŠŸèƒ½ï¼šåˆ†æåŸºçº¿å®éªŒå’Œåˆ›æ–°ç‚¹å®éªŒçš„ç»“æœå¹¶ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    æ’¤é”€æ–¹æ³•ï¼šåˆ é™¤æ­¤å‡½æ•°
    """
    parser = argparse.ArgumentParser(description="åˆ†æMambaProå®éªŒç»“æœ")
    parser.add_argument("--baseline_log", required=True, help="åŸºçº¿å®éªŒæ—¥å¿—æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--moe_log", required=True, help="åˆ›æ–°ç‚¹å®éªŒæ—¥å¿—æ–‡ä»¶è·¯å¾„")
    parser.add_argument("--output_dir", required=True, help="è¾“å‡ºç›®å½•")
    
    args = parser.parse_args()
    
    print("ğŸ” MambaPro å®éªŒç»“æœåˆ†æ")
    print("=" * 50)
    print(f"ğŸ“… åˆ†ææ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}")
    
    # åˆ†æåŸºçº¿å®éªŒç»“æœ
    print("\nğŸ“Š åˆ†æåŸºçº¿å®éªŒç»“æœ...")
    baseline_metrics = extract_final_metrics(args.baseline_log)
    
    # åˆ†æåˆ›æ–°ç‚¹å®éªŒç»“æœ
    print("ğŸ“Š åˆ†æåˆ›æ–°ç‚¹å®éªŒç»“æœ...")
    moe_metrics = extract_final_metrics(args.moe_log)
    
    # ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Š
    if baseline_metrics and moe_metrics:
        generate_comparison_report(baseline_metrics, moe_metrics, args.output_dir)
    else:
        print("âŒ æ— æ³•ç”Ÿæˆå¯¹æ¯”æŠ¥å‘Šï¼šç¼ºå°‘å¿…è¦çš„æŒ‡æ ‡æ•°æ®")

if __name__ == "__main__":
    main()
