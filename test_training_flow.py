"""
è®­ç»ƒæµç¨‹æµ‹è¯•è„šæœ¬

åŠŸèƒ½ï¼š
- æ¨¡æ‹Ÿå®Œæ•´çš„è®­ç»ƒæµç¨‹
- æ£€æŸ¥ç»´åº¦å’Œå‚æ•°åŒ¹é…
- ç¡®ä¿ä¸ä¼šæŠ¥é”™

ä½œè€…ï¼šç”¨æˆ·ä¿®æ”¹
æ—¥æœŸï¼š2024
"""

import torch
import torch.nn as nn
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def test_clip_multi_scale_module():
    """æµ‹è¯•CLIPå¤šå°ºåº¦æ»‘åŠ¨çª—å£æ¨¡å—"""
    print("=== æµ‹è¯•CLIPå¤šå°ºåº¦æ»‘åŠ¨çª—å£æ¨¡å— ===")
    
    try:
        from modeling.fusion_part.clip_multi_scale_sliding_window import CLIPMultiScaleFeatureExtractor
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        batch_size = 2
        seq_len = 196  # 14x14 patches
        feat_dim = 512  # CLIPç‰¹å¾ç»´åº¦
        
        # åˆ›å»ºæ¨¡å‹
        model = CLIPMultiScaleFeatureExtractor(feat_dim=feat_dim, scales=[4, 8, 16])
        
        # åˆ›å»ºæµ‹è¯•è¾“å…¥
        patch_tokens = torch.randn(batch_size, seq_len, feat_dim)
        
        print(f"è¾“å…¥å½¢çŠ¶: {patch_tokens.shape}")
        
        # å‰å‘ä¼ æ’­
        with torch.no_grad():
            multi_scale_feature = model(patch_tokens)
        
        print(f"è¾“å‡ºå½¢çŠ¶: {multi_scale_feature.shape}")
        print(f"æœŸæœ›å½¢çŠ¶: [{batch_size}, {feat_dim}]")
        
        # éªŒè¯è¾“å‡ºå½¢çŠ¶
        assert multi_scale_feature.shape == (batch_size, feat_dim), f"è¾“å‡ºå½¢çŠ¶ä¸åŒ¹é…: {multi_scale_feature.shape}"
        
        print("âœ… CLIPå¤šå°ºåº¦æ»‘åŠ¨çª—å£æ¨¡å—æµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"âŒ CLIPå¤šå°ºåº¦æ»‘åŠ¨çª—å£æ¨¡å—æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_build_transformer_initialization():
    """æµ‹è¯•build_transformeråˆå§‹åŒ–"""
    print("\n=== æµ‹è¯•build_transformeråˆå§‹åŒ– ===")
    
    try:
        # æ¨¡æ‹Ÿé…ç½®
        class MockConfig:
            def __init__(self):
                self.MODEL = MockModel()
                self.INPUT = MockInput()
        
        class MockModel:
            def __init__(self):
                self.TRANSFORMER_TYPE = 'ViT-B-16'
                self.SIE_COE = 1.0
                self.SIE_CAMERA = True
                self.SIE_VIEW = False
                self.FROZEN = True
                self.USE_CLIP_MULTI_SCALE = True
                self.CLIP_MULTI_SCALE_SCALES = [4, 8, 16]
        
        class MockInput:
            def __init__(self):
                self.SIZE_TRAIN = [256, 128]
        
        mock_cfg = MockConfig()
        
        # æ¨¡æ‹Ÿå‚æ•°
        num_classes = 100
        camera_num = 6
        view_num = 0
        feat_dim = 512
        
        print(f"é…ç½®: TRANSFORMER_TYPE = {mock_cfg.MODEL.TRANSFORMER_TYPE}")
        print(f"å‚æ•°: num_classes = {num_classes}, camera_num = {camera_num}, feat_dim = {feat_dim}")
        
        # éªŒè¯é…ç½®è¯»å–
        use_clip_multi_scale = getattr(mock_cfg.MODEL, 'USE_CLIP_MULTI_SCALE', False)
        clip_scales = getattr(mock_cfg.MODEL, 'CLIP_MULTI_SCALE_SCALES', [4, 8, 16])
        
        print(f"USE_CLIP_MULTI_SCALE: {use_clip_multi_scale}")
        print(f"CLIP_MULTI_SCALE_SCALES: {clip_scales}")
        
        assert use_clip_multi_scale == True, "é…ç½®è¯»å–å¤±è´¥"
        assert clip_scales == [4, 8, 16], "é…ç½®è¯»å–å¤±è´¥"
        
        print("âœ… build_transformeråˆå§‹åŒ–æµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"âŒ build_transformeråˆå§‹åŒ–æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_mambapro_initialization():
    """æµ‹è¯•MambaProåˆå§‹åŒ–"""
    print("\n=== æµ‹è¯•MambaProåˆå§‹åŒ– ===")
    
    try:
        # æ¨¡æ‹Ÿé…ç½®
        class MockConfig:
            def __init__(self):
                self.MODEL = MockModel()
                self.DATALOADER = MockDataloader()
                self.TEST = MockTest()
        
        class MockModel:
            def __init__(self):
                self.TRANSFORMER_TYPE = 'ViT-B-16'
                self.DIRECT = 1
                self.NECK = 'bnneck'
                self.ID_LOSS_TYPE = 'arcsoftmax'
                self.MAMBA = True
        
        class MockDataloader:
            def __init__(self):
                self.NUM_INSTANCE = 8
        
        class MockTest:
            def __init__(self):
                self.NECK_FEAT = 'after'
                self.MISS = 'nothing'
        
        mock_cfg = MockConfig()
        
        # æ¨¡æ‹Ÿå‚æ•°
        num_classes = 100
        camera_num = 6
        view_num = 0
        
        print(f"é…ç½®: TRANSFORMER_TYPE = {mock_cfg.MODEL.TRANSFORMER_TYPE}")
        print(f"å‚æ•°: num_classes = {num_classes}, camera_num = {camera_num}")
        
        # éªŒè¯ç‰¹å¾ç»´åº¦è®¾ç½®
        if 'ViT-B-16' in mock_cfg.MODEL.TRANSFORMER_TYPE:
            feat_dim = 512  # CLIP ViT-B/16 ç»´åº¦
        else:
            feat_dim = 768  # ViT åŸºæœ¬ç»´åº¦
        
        print(f"ç‰¹å¾ç»´åº¦: {feat_dim}")
        
        assert feat_dim == 512, f"ç‰¹å¾ç»´åº¦é”™è¯¯: {feat_dim}"
        
        print("âœ… MambaProåˆå§‹åŒ–æµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"âŒ MambaProåˆå§‹åŒ–æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_forward_pass_simulation():
    """æµ‹è¯•å®Œæ•´å‰å‘ä¼ æ’­æ¨¡æ‹Ÿ"""
    print("\n=== æµ‹è¯•å®Œæ•´å‰å‘ä¼ æ’­æ¨¡æ‹Ÿ ===")
    
    try:
        # æ¨¡æ‹Ÿå®Œæ•´çš„CLIPå¤šå°ºåº¦å‰å‘ä¼ æ’­
        batch_size = 2
        seq_len = 196
        feat_dim = 512
        
        # 1. æ¨¡æ‹ŸCLIPå‰å‘ä¼ æ’­
        clip_output = torch.randn(batch_size, seq_len + 1, feat_dim)
        print(f"1. CLIPè¾“å‡º: {clip_output.shape}")
        
        # 2. åˆ†ç¦»tokens
        cls_token = clip_output[:, 0:1, :]
        patch_tokens = clip_output[:, 1:, :]
        print(f"2. åˆ†ç¦»tokens - CLS: {cls_token.shape}, Patches: {patch_tokens.shape}")
        
        # 3. å¤šå°ºåº¦å¤„ç†
        from modeling.fusion_part.clip_multi_scale_sliding_window import CLIPMultiScaleFeatureExtractor
        multi_scale_extractor = CLIPMultiScaleFeatureExtractor(feat_dim=feat_dim, scales=[4, 8, 16])
        
        with torch.no_grad():
            multi_scale_feature = multi_scale_extractor(patch_tokens)
        print(f"3. å¤šå°ºåº¦ç‰¹å¾: {multi_scale_feature.shape}")
        
        # 4. ç‰¹å¾èåˆ
        enhanced_cls = cls_token + multi_scale_feature.unsqueeze(1)
        enhanced_output = torch.cat([enhanced_cls, patch_tokens], dim=1)
        print(f"4. å¢å¼ºè¾“å‡º: {enhanced_output.shape}")
        
        # 5. å…¨å±€ç‰¹å¾æå–
        global_feat = enhanced_output[:, 0]  # [B, 512]
        print(f"5. å…¨å±€ç‰¹å¾: {global_feat.shape}")
        
        # 6. ä¸‰æ¨¡æ€æ‹¼æ¥
        RGB_global = global_feat
        NI_global = global_feat
        TI_global = global_feat
        ori = torch.cat([RGB_global, NI_global, TI_global], dim=-1)  # [B, 1536]
        print(f"6. ä¸‰æ¨¡æ€æ‹¼æ¥: {ori.shape}")
        
        # 7. BNNeckå’Œåˆ†ç±»
        bottleneck = nn.BatchNorm1d(3 * feat_dim)
        classifier = nn.Linear(3 * feat_dim, 100)  # å‡è®¾100ä¸ªç±»åˆ«
        
        with torch.no_grad():
            ori_global = bottleneck(ori)
            ori_score = classifier(ori_global)
        
        print(f"7. BNNeckè¾“å‡º: {ori_global.shape}")
        print(f"8. åˆ†ç±»è¾“å‡º: {ori_score.shape}")
        
        # éªŒè¯æ‰€æœ‰å½¢çŠ¶
        assert global_feat.shape == (batch_size, feat_dim), f"å…¨å±€ç‰¹å¾å½¢çŠ¶é”™è¯¯: {global_feat.shape}"
        assert ori.shape == (batch_size, 3 * feat_dim), f"ä¸‰æ¨¡æ€æ‹¼æ¥å½¢çŠ¶é”™è¯¯: {ori.shape}"
        assert ori_score.shape == (batch_size, 100), f"åˆ†ç±»è¾“å‡ºå½¢çŠ¶é”™è¯¯: {ori_score.shape}"
        
        print("âœ… å®Œæ•´å‰å‘ä¼ æ’­æ¨¡æ‹Ÿæµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"âŒ å®Œæ•´å‰å‘ä¼ æ’­æ¨¡æ‹Ÿæµ‹è¯•å¤±è´¥: {e}")
        return False

def test_dimension_consistency():
    """æµ‹è¯•ç»´åº¦ä¸€è‡´æ€§"""
    print("\n=== æµ‹è¯•ç»´åº¦ä¸€è‡´æ€§ ===")
    
    try:
        # æ£€æŸ¥å…³é”®ç»´åº¦
        batch_size = 2
        seq_len = 196
        feat_dim = 512
        
        print(f"æ‰¹æ¬¡å¤§å°: {batch_size}")
        print(f"åºåˆ—é•¿åº¦: {seq_len}")
        print(f"ç‰¹å¾ç»´åº¦: {feat_dim}")
        
        # 1. CLIPè¾“å‡ºç»´åº¦
        clip_output = torch.randn(batch_size, seq_len + 1, feat_dim)
        print(f"CLIPè¾“å‡º: {clip_output.shape}")
        
        # 2. å¤šå°ºåº¦ç‰¹å¾ç»´åº¦
        from modeling.fusion_part.clip_multi_scale_sliding_window import CLIPMultiScaleFeatureExtractor
        multi_scale_extractor = CLIPMultiScaleFeatureExtractor(feat_dim=feat_dim, scales=[4, 8, 16])
        
        patch_tokens = clip_output[:, 1:, :]
        with torch.no_grad():
            multi_scale_feature = multi_scale_extractor(patch_tokens)
        print(f"å¤šå°ºåº¦ç‰¹å¾: {multi_scale_feature.shape}")
        
        # 3. ä¸‰æ¨¡æ€æ‹¼æ¥ç»´åº¦
        ori = torch.cat([multi_scale_feature, multi_scale_feature, multi_scale_feature], dim=-1)
        print(f"ä¸‰æ¨¡æ€æ‹¼æ¥: {ori.shape}")
        
        # 4. åˆ†ç±»å¤´ç»´åº¦
        num_classes = 100
        classifier = nn.Linear(3 * feat_dim, num_classes)
        with torch.no_grad():
            score = classifier(ori)
        print(f"åˆ†ç±»è¾“å‡º: {score.shape}")
        
        # éªŒè¯ç»´åº¦ä¸€è‡´æ€§
        assert multi_scale_feature.shape == (batch_size, feat_dim), "å¤šå°ºåº¦ç‰¹å¾ç»´åº¦é”™è¯¯"
        assert ori.shape == (batch_size, 3 * feat_dim), "ä¸‰æ¨¡æ€æ‹¼æ¥ç»´åº¦é”™è¯¯"
        assert score.shape == (batch_size, num_classes), "åˆ†ç±»è¾“å‡ºç»´åº¦é”™è¯¯"
        
        print("âœ… ç»´åº¦ä¸€è‡´æ€§æµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"âŒ ç»´åº¦ä¸€è‡´æ€§æµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹è®­ç»ƒæµç¨‹æµ‹è¯•")
    print("=" * 60)
    
    test_results = []
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    test_results.append(test_clip_multi_scale_module())
    test_results.append(test_build_transformer_initialization())
    test_results.append(test_mambapro_initialization())
    test_results.append(test_forward_pass_simulation())
    test_results.append(test_dimension_consistency())
    
    # ç»Ÿè®¡ç»“æœ
    passed = sum(test_results)
    total = len(test_results)
    
    print("\n" + "=" * 60)
    print(f"ğŸ“Š æµ‹è¯•ç»“æœç»Ÿè®¡: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼è®­ç»ƒæµç¨‹éªŒè¯æˆåŠŸï¼")
        print("\nâœ… åŠŸèƒ½ç¡®è®¤:")
        print("   - CLIPå¤šå°ºåº¦æ»‘åŠ¨çª—å£æ¨¡å—å·¥ä½œæ­£å¸¸")
        print("   - build_transformeråˆå§‹åŒ–æ­£ç¡®")
        print("   - MambaProåˆå§‹åŒ–æ­£ç¡®")
        print("   - å‰å‘ä¼ æ’­æµç¨‹æ­£ç¡®")
        print("   - ç»´åº¦ä¸€è‡´æ€§éªŒè¯é€šè¿‡")
        print("\nğŸ¯ ç°åœ¨å¯ä»¥å®‰å…¨è¿è¡Œè®­ç»ƒå‘½ä»¤:")
        print("   python train_net.py --config_file configs/RGBNT201/MambaPro.yml")
        print("\nğŸ“‹ é¢„æœŸè¾“å‡º:")
        print("   Loading pretrained model from CLIP")
        print("   âœ… ä¸ºCLIPå¯ç”¨å¤šå°ºåº¦æ»‘åŠ¨çª—å£ç‰¹å¾æå–æ¨¡å—")
        print("   - æ»‘åŠ¨çª—å£å°ºåº¦: [4, 8, 16]")
        print("   - ç‰¹å¾ç»´åº¦: 512 (CLIP)")
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä»£ç å®ç°")
    
    return passed == total

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
