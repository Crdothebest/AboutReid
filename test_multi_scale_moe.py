"""
æµ‹è¯•å¤šå°ºåº¦MoEæ¨¡å—çš„åŠŸèƒ½
"""

import torch
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append('/home/zubuntu/workspace/yzy/MambaPro')

def test_multi_scale_moe():
    """æµ‹è¯•å¤šå°ºåº¦MoEæ¨¡å—"""
    print("ğŸ§ª å¼€å§‹æµ‹è¯•å¤šå°ºåº¦MoEæ¨¡å—...")
    
    try:
        from modeling.fusion_part.multi_scale_moe import MultiScaleMoE, MultiScaleMoEAAM
        
        # æµ‹è¯•å‚æ•°
        batch_size = 2
        seq_len = 197  # ViTçš„patchæ•°é‡ (14*14 + 1)
        dim = 512      # CLIP ViT-B/16çš„ç»´åº¦
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        x = torch.randn(batch_size, seq_len, dim)
        print(f"âœ… è¾“å…¥æ•°æ®å½¢çŠ¶: {x.shape}")
        
        # æµ‹è¯•MultiScaleMoEæ¨¡å—
        moe_module = MultiScaleMoE(dim=dim, scales=[4, 8, 16])
        output = moe_module(x)
        print(f"âœ… MultiScaleMoEè¾“å‡ºå½¢çŠ¶: {output.shape}")
        
        # æµ‹è¯•MultiScaleMoEAAMæ¨¡å—
        # æ¨¡æ‹Ÿé…ç½®
        class MockConfig:
            MODEL = type('obj', (object,), {
                'MAMBA_BI': False,
                'TRANSFORMER_TYPE': 'ViT-B-16'
            })()
        
        cfg = MockConfig()
        aam_module = MultiScaleMoEAAM(dim=dim, n_layers=2, cfg=cfg)
        
        # åˆ›å»ºä¸‰ç§æ¨¡æ€çš„æµ‹è¯•æ•°æ®
        r = torch.randn(batch_size, seq_len, dim)
        n = torch.randn(batch_size, seq_len, dim)
        t = torch.randn(batch_size, seq_len, dim)
        
        output = aam_module(r, n, t)
        print(f"âœ… MultiScaleMoEAAMè¾“å‡ºå½¢çŠ¶: {output.shape}")
        
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼å¤šå°ºåº¦MoEæ¨¡å—å·¥ä½œæ­£å¸¸ã€‚")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()

if __name__ == "__main__":
    test_multi_scale_moe()
