"""
CLIPå¤šå°ºåº¦æ»‘åŠ¨çª—å£é›†æˆæµ‹è¯•è„šæœ¬

åŠŸèƒ½ï¼š
- æµ‹è¯•CLIPå¤šå°ºåº¦æ»‘åŠ¨çª—å£æ¨¡å—çš„åŠŸèƒ½
- éªŒè¯ä¸CLIPåˆ†æ”¯çš„é›†æˆ
- ç¡®ä¿åœ¨ä¿æŒCLIPåˆ†æ”¯å®Œæ•´æ€§çš„åŸºç¡€ä¸Šæ·»åŠ å¤šå°ºåº¦åŠŸèƒ½

ä½œè€…ï¼šç”¨æˆ·ä¿®æ”¹
æ—¥æœŸï¼š2024
"""

import torch
import torch.nn as nn
import sys
import os

# æ·»åŠ é¡¹ç›®è·¯å¾„
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

def test_clip_multi_scale_module():
    """æµ‹è¯•CLIPå¤šå°ºåº¦æ»‘åŠ¨çª—å£æ¨¡å—"""
    print("=== æµ‹è¯•CLIPå¤šå°ºåº¦æ»‘åŠ¨çª—å£æ¨¡å— ===")
    
    try:
        from modeling.fusion_part.clip_multi_scale_sliding_window import CLIPMultiScaleFeatureExtractor
        
        # åˆ›å»ºæµ‹è¯•æ•°æ®
        batch_size = 2
        seq_len = 196  # 14x14 patches
        feat_dim = 512  # CLIPç‰¹å¾ç»´åº¦
        
        # åˆ›å»ºæ¨¡å‹
        model = CLIPMultiScaleFeatureExtractor(feat_dim=feat_dim, scales=[4, 8, 16])
        
        # åˆ›å»ºæµ‹è¯•è¾“å…¥
        patch_tokens = torch.randn(batch_size, seq_len, feat_dim)
        
        print(f"è¾“å…¥å½¢çŠ¶: {patch_tokens.shape}")
        
        # å‰å‘ä¼ æ’­
        with torch.no_grad():
            multi_scale_feature = model(patch_tokens)
        
        print(f"è¾“å‡ºå½¢çŠ¶: {multi_scale_feature.shape}")
        print(f"æœŸæœ›å½¢çŠ¶: [{batch_size}, {feat_dim}]")
        
        # éªŒè¯è¾“å‡ºå½¢çŠ¶
        assert multi_scale_feature.shape == (batch_size, feat_dim), f"è¾“å‡ºå½¢çŠ¶ä¸åŒ¹é…: {multi_scale_feature.shape}"
        
        print("âœ… CLIPå¤šå°ºåº¦æ»‘åŠ¨çª—å£æ¨¡å—æµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"âŒ CLIPå¤šå°ºåº¦æ»‘åŠ¨çª—å£æ¨¡å—æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_clip_branch_integration():
    """æµ‹è¯•CLIPåˆ†æ”¯é›†æˆ"""
    print("\n=== æµ‹è¯•CLIPåˆ†æ”¯é›†æˆ ===")
    
    try:
        # æ¨¡æ‹ŸCLIPåˆ†æ”¯çš„å‰å‘ä¼ æ’­
        batch_size = 2
        seq_len = 196
        feat_dim = 512
        
        # æ¨¡æ‹ŸCLIPè¾“å‡º
        clip_output = torch.randn(batch_size, seq_len + 1, feat_dim)  # [B, N+1, 512]
        
        print(f"CLIPè¾“å‡ºå½¢çŠ¶: {clip_output.shape}")
        
        # åˆ†ç¦»CLS tokenå’Œpatch tokens
        cls_token = clip_output[:, 0:1, :]  # [B, 1, 512]
        patch_tokens = clip_output[:, 1:, :]  # [B, N, 512]
        
        print(f"CLS tokenå½¢çŠ¶: {cls_token.shape}")
        print(f"Patch tokenså½¢çŠ¶: {patch_tokens.shape}")
        
        # å¯¼å…¥å¤šå°ºåº¦æ¨¡å—
        from modeling.fusion_part.clip_multi_scale_sliding_window import CLIPMultiScaleFeatureExtractor
        multi_scale_extractor = CLIPMultiScaleFeatureExtractor(feat_dim=feat_dim, scales=[4, 8, 16])
        
        # å¤šå°ºåº¦å¤„ç†
        with torch.no_grad():
            multi_scale_feature = multi_scale_extractor(patch_tokens)  # [B, 512]
        
        print(f"å¤šå°ºåº¦ç‰¹å¾å½¢çŠ¶: {multi_scale_feature.shape}")
        
        # ç‰¹å¾èåˆ
        enhanced_cls = cls_token + multi_scale_feature.unsqueeze(1)  # [B, 1, 512]
        enhanced_output = torch.cat([enhanced_cls, patch_tokens], dim=1)  # [B, N+1, 512]
        
        print(f"å¢å¼ºè¾“å‡ºå½¢çŠ¶: {enhanced_output.shape}")
        
        # éªŒè¯å½¢çŠ¶
        assert enhanced_output.shape == clip_output.shape, f"å½¢çŠ¶ä¸åŒ¹é…: {enhanced_output.shape} vs {clip_output.shape}"
        
        print("âœ… CLIPåˆ†æ”¯é›†æˆæµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"âŒ CLIPåˆ†æ”¯é›†æˆæµ‹è¯•å¤±è´¥: {e}")
        return False

def test_config_loading():
    """æµ‹è¯•é…ç½®åŠ è½½"""
    print("\n=== æµ‹è¯•é…ç½®åŠ è½½ ===")
    
    try:
        from config import cfg
        
        # æ¨¡æ‹Ÿé…ç½®
        class MockConfig:
            def __init__(self):
                self.MODEL = MockModel()
        
        class MockModel:
            def __init__(self):
                self.USE_CLIP_MULTI_SCALE = True
                self.CLIP_MULTI_SCALE_SCALES = [4, 8, 16]
        
        mock_cfg = MockConfig()
        
        # æµ‹è¯•é…ç½®è¯»å–
        use_clip_multi_scale = getattr(mock_cfg.MODEL, 'USE_CLIP_MULTI_SCALE', False)
        clip_scales = getattr(mock_cfg.MODEL, 'CLIP_MULTI_SCALE_SCALES', [4, 8, 16])
        
        print(f"USE_CLIP_MULTI_SCALE: {use_clip_multi_scale}")
        print(f"CLIP_MULTI_SCALE_SCALES: {clip_scales}")
        
        assert use_clip_multi_scale == True, "é…ç½®è¯»å–å¤±è´¥"
        assert clip_scales == [4, 8, 16], "é…ç½®è¯»å–å¤±è´¥"
        
        print("âœ… é…ç½®åŠ è½½æµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"âŒ é…ç½®åŠ è½½æµ‹è¯•å¤±è´¥: {e}")
        return False

def test_forward_pass_simulation():
    """æµ‹è¯•å®Œæ•´å‰å‘ä¼ æ’­æ¨¡æ‹Ÿ"""
    print("\n=== æµ‹è¯•å®Œæ•´å‰å‘ä¼ æ’­æ¨¡æ‹Ÿ ===")
    
    try:
        # æ¨¡æ‹Ÿå®Œæ•´çš„CLIPå¤šå°ºåº¦å‰å‘ä¼ æ’­
        batch_size = 2
        seq_len = 196
        feat_dim = 512
        
        # 1. æ¨¡æ‹ŸCLIPå‰å‘ä¼ æ’­
        clip_output = torch.randn(batch_size, seq_len + 1, feat_dim)
        print(f"1. CLIPè¾“å‡º: {clip_output.shape}")
        
        # 2. åˆ†ç¦»tokens
        cls_token = clip_output[:, 0:1, :]
        patch_tokens = clip_output[:, 1:, :]
        print(f"2. åˆ†ç¦»tokens - CLS: {cls_token.shape}, Patches: {patch_tokens.shape}")
        
        # 3. å¤šå°ºåº¦å¤„ç†
        from modeling.fusion_part.clip_multi_scale_sliding_window import CLIPMultiScaleFeatureExtractor
        multi_scale_extractor = CLIPMultiScaleFeatureExtractor(feat_dim=feat_dim, scales=[4, 8, 16])
        
        with torch.no_grad():
            multi_scale_feature = multi_scale_extractor(patch_tokens)
        print(f"3. å¤šå°ºåº¦ç‰¹å¾: {multi_scale_feature.shape}")
        
        # 4. ç‰¹å¾èåˆ
        enhanced_cls = cls_token + multi_scale_feature.unsqueeze(1)
        enhanced_output = torch.cat([enhanced_cls, patch_tokens], dim=1)
        print(f"4. å¢å¼ºè¾“å‡º: {enhanced_output.shape}")
        
        # 5. å…¨å±€ç‰¹å¾æå–
        global_feat = enhanced_output[:, 0]  # [B, 512]
        print(f"5. å…¨å±€ç‰¹å¾: {global_feat.shape}")
        
        # éªŒè¯æ‰€æœ‰å½¢çŠ¶
        assert global_feat.shape == (batch_size, feat_dim), f"å…¨å±€ç‰¹å¾å½¢çŠ¶é”™è¯¯: {global_feat.shape}"
        
        print("âœ… å®Œæ•´å‰å‘ä¼ æ’­æ¨¡æ‹Ÿæµ‹è¯•é€šè¿‡ï¼")
        return True
        
    except Exception as e:
        print(f"âŒ å®Œæ•´å‰å‘ä¼ æ’­æ¨¡æ‹Ÿæµ‹è¯•å¤±è´¥: {e}")
        return False

def main():
    """ä¸»æµ‹è¯•å‡½æ•°"""
    print("ğŸš€ å¼€å§‹CLIPå¤šå°ºåº¦æ»‘åŠ¨çª—å£é›†æˆæµ‹è¯•")
    print("=" * 60)
    
    test_results = []
    
    # è¿è¡Œæ‰€æœ‰æµ‹è¯•
    test_results.append(test_clip_multi_scale_module())
    test_results.append(test_clip_branch_integration())
    test_results.append(test_config_loading())
    test_results.append(test_forward_pass_simulation())
    
    # ç»Ÿè®¡ç»“æœ
    passed = sum(test_results)
    total = len(test_results)
    
    print("\n" + "=" * 60)
    print(f"ğŸ“Š æµ‹è¯•ç»“æœç»Ÿè®¡: {passed}/{total} é€šè¿‡")
    
    if passed == total:
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼CLIPå¤šå°ºåº¦æ»‘åŠ¨çª—å£é›†æˆæˆåŠŸï¼")
        print("\nâœ… åŠŸèƒ½ç¡®è®¤:")
        print("   - CLIPå¤šå°ºåº¦æ»‘åŠ¨çª—å£æ¨¡å—å·¥ä½œæ­£å¸¸")
        print("   - CLIPåˆ†æ”¯é›†æˆæˆåŠŸ")
        print("   - é…ç½®åŠ è½½æ­£å¸¸")
        print("   - å‰å‘ä¼ æ’­æµç¨‹æ­£ç¡®")
        print("\nğŸ¯ ç°åœ¨å¯ä»¥è¿è¡Œè®­ç»ƒå‘½ä»¤:")
        print("   python train_net.py --config_file configs/RGBNT201/MambaPro.yml")
    else:
        print("âŒ éƒ¨åˆ†æµ‹è¯•å¤±è´¥ï¼Œè¯·æ£€æŸ¥ä»£ç å®ç°")
    
    return passed == total

if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
