from utils.logger import setup_logger
from data import make_dataloader
from modeling import make_model
from solver.make_optimizer import make_optimizer
from solver.scheduler_factory import create_scheduler
from layers.make_loss import make_loss
from engine.processor import do_train
import random
import torch
import numpy as np
import os
import argparse
from config import cfg


def set_seed(seed):
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)
    np.random.seed(seed)
    random.seed(seed)
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = True

# è®­ç»ƒä¸»å‡½æ•°
if __name__ == '__main__':

    parser = argparse.ArgumentParser(description="MambaPro Training") # åˆ›å»ºå‘½ä»¤è¡Œè§£æå™¨
    parser.add_argument( # æ·»åŠ é…ç½®æ–‡ä»¶è·¯å¾„å‚æ•°
        "--config_file", default="/home/zubuntu/workspace/yzy/MambaPro/configs/MSVR310/MambaPro.yml", help="path to config file", type=str
    )# é»˜è®¤é…ç½®æ–‡ä»¶è·¯å¾„
    parser.add_argument("--fea_cft", default=0, help="Feature choose to be tested", type=int) # æ·»åŠ ç‰¹å¾é€‰æ‹©å‚æ•°
    parser.add_argument("opts", help="Modify config options using the command-line", default=None,
                        nargs=argparse.REMAINDER) # æ·»åŠ å‘½ä»¤è¡Œå‚æ•°
    parser.add_argument("--local_rank", default=0, type=int) # æ·»åŠ æœ¬åœ°æ’åå‚æ•°
    # ğŸ”¥ æ–°å¢ï¼šå¤šå°ºåº¦æ»‘åŠ¨çª—å£æ§åˆ¶å‚æ•°
    parser.add_argument("--use_multi_scale", action="store_true", help="Enable multi-scale sliding window (default: False)")
    parser.add_argument("--no_multi_scale", action="store_true", help="Disable multi-scale sliding window (default: False)")
    args = parser.parse_args() # è§£æå‚æ•°

    if args.config_file != "":
        cfg.merge_from_file(args.config_file) # ä»é…ç½®æ–‡ä»¶åˆå¹¶é…ç½®
    cfg.merge_from_list(args.opts) # ä»å‘½ä»¤è¡Œåˆå¹¶é…ç½®
    cfg.TEST.FEAT = args.fea_cft # è®¾ç½®ç‰¹å¾é€‰æ‹©
    
    # ğŸ”¥ æ–°å¢ï¼šå¤„ç†å¤šå°ºåº¦æ»‘åŠ¨çª—å£å‘½ä»¤è¡Œå‚æ•°
    if args.use_multi_scale:
        cfg.MODEL.USE_CLIP_MULTI_SCALE = True
        print("ğŸ”¥ å¯ç”¨å¤šå°ºåº¦æ»‘åŠ¨çª—å£ (å‘½ä»¤è¡Œå‚æ•°)")
    elif args.no_multi_scale:
        cfg.MODEL.USE_CLIP_MULTI_SCALE = False
        print("ğŸ”¥ ç¦ç”¨å¤šå°ºåº¦æ»‘åŠ¨çª—å£ (å‘½ä»¤è¡Œå‚æ•°)")
    else:
        # ä½¿ç”¨é…ç½®æ–‡ä»¶ä¸­çš„é»˜è®¤å€¼
        print(f"ğŸ”¥ ä½¿ç”¨é…ç½®æ–‡ä»¶è®¾ç½®: USE_CLIP_MULTI_SCALE = {cfg.MODEL.USE_CLIP_MULTI_SCALE}")
    
    cfg.freeze() # å†»ç»“é…ç½®

    set_seed(cfg.SOLVER.SEED) # è®¾ç½®éšæœºç§å­
    
    if cfg.MODEL.DIST_TRAIN: # å¦‚æœä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒ
        torch.cuda.set_device(args.local_rank) # è®¾ç½®æœ¬åœ°æ’å

    output_dir = cfg.OUTPUT_DIR # è®¾ç½®è¾“å‡ºç›®å½•
    if output_dir and not os.path.exists(output_dir): # å¦‚æœè¾“å‡ºç›®å½•ä¸å­˜åœ¨
        os.makedirs(output_dir) # åˆ›å»ºè¾“å‡ºç›®å½•

    logger = setup_logger("MambaPro", output_dir, if_train=True) # è®¾ç½®æ—¥å¿—
    logger.info("Saving model in the path :{}".format(cfg.OUTPUT_DIR)) # æ‰“å°è¾“å‡ºç›®å½•
    logger.info(args) # æ‰“å°å‚æ•°

    if args.config_file != "":
        logger.info("Loaded configuration file {}".format(args.config_file)) # æ‰“å°åŠ è½½çš„é…ç½®æ–‡ä»¶
        with open(args.config_file, 'r') as cf: # æ‰“å¼€é…ç½®æ–‡ä»¶
            config_str = "\n" + cf.read() # è¯»å–é…ç½®æ–‡ä»¶
            logger.info(config_str) # æ‰“å°é…ç½®æ–‡ä»¶
    logger.info("Running with config:\n{}".format(cfg)) # æ‰“å°é…ç½®

    if cfg.MODEL.DIST_TRAIN: # å¦‚æœä½¿ç”¨åˆ†å¸ƒå¼è®­ç»ƒ
        torch.distributed.init_process_group(backend='nccl', init_method='env://') # åˆå§‹åŒ–åˆ†å¸ƒå¼è®­ç»ƒ

    os.environ['CUDA_VISIBLE_DEVICES'] = cfg.MODEL.DEVICE_ID # è®¾ç½®å¯è§è®¾å¤‡
    train_loader, train_loader_normal, val_loader, num_query, num_classes, camera_num, view_num = make_dataloader(cfg) # åŠ è½½æ•°æ®
    print("data is ready") # æ‰“å°æ•°æ®åŠ è½½å®Œæˆ
    model = make_model(cfg, num_class=num_classes, camera_num=camera_num, view_num=view_num) # åŠ è½½æ¨¡å‹

    loss_func, center_criterion = make_loss(cfg, num_classes=num_classes) # åŠ è½½æŸå¤±å‡½æ•°

    optimizer, optimizer_center = make_optimizer(cfg, model, center_criterion) # åŠ è½½ä¼˜åŒ–å™¨

    scheduler = create_scheduler(cfg, optimizer) # åŠ è½½è°ƒåº¦å™¨
    do_train(
        cfg, # é…ç½®
        model, # æ¨¡å‹
        center_criterion, # ä¸­å¿ƒæŸå¤±
        train_loader, # è®­ç»ƒæ•°æ®
        val_loader, # éªŒè¯æ•°æ®
        optimizer, # ä¼˜åŒ–å™¨
        optimizer_center, # ä¸­å¿ƒä¼˜åŒ–å™¨
        scheduler, # è°ƒåº¦å™¨
        loss_func, # æŸå¤±å‡½æ•°
        num_query, args.local_rank # æŸ¥è¯¢æ•°é‡å’Œæœ¬åœ°æ’å
    )
