MODEL:
  PRETRAIN_PATH_T: '/home/zubuntu/workspace/yzy/MambaPro/pths/ViT-B-16.pt' # 预训练权重
  TRANSFORMER_TYPE: 'ViT-B-16' # 骨干类型
  STRIDE_SIZE: [ 16, 16 ] # 步长
  SIE_CAMERA: True # 是否使用相机嵌入
  DIRECT: 1 # 直接
  SIE_COE: 1.0 # 相机嵌入系数
  ID_LOSS_WEIGHT: 0.25 # 分类损失权重
  TRIPLET_LOSS_WEIGHT: 1.0 # 三元组损失权重
  PROMPT: True # 是否使用提示
  ADAPTER: True # 是否使用适配器
  MAMBA: True # 是否使用Mamba
  FROZEN: True # 是否冻结
  
  # ========== 多尺度滑动窗口配置：启用多尺度特征提取 ==========
  # 实验目的：启用多尺度滑动窗口，为MoE提供多尺度特征输入
  # 功能：基于CLIP的多尺度特征提取机制
  # 🔥 核心配置：启用多尺度滑动窗口，提供4×4、8×8、16×16三个尺度特征
  USE_CLIP_MULTI_SCALE: True   # 启用CLIP多尺度滑动窗口特征提取
  CLIP_MULTI_SCALE_SCALES: [4, 8, 16]  # 滑动窗口尺度（4x4小窗口，8x8中窗口，16x16大窗口）
  
  # ========== 最优MoE配置：基于经验的精心调优参数 ==========
  # 实验目的：启用并优化MoE模块，实现最佳性能
  # 功能：基于专家网络的多尺度特征动态融合机制
  # 🔥 核心配置：启用MoE融合，使用最优参数组合
  USE_MULTI_SCALE_MOE: True    # 启用多尺度MoE特征融合模块
  MOE_SCALES: [4, 8, 16]       # MoE滑动窗口尺度（与多尺度滑动窗口保持一致）
  
  # ========== 最优MoE参数：基于经验的精心调优组合 ==========
  # 🏆 专家网络隐藏层维度：1024维（经验最优值）
  # 原理：1024维在容量和效率之间达到最佳平衡
  # 经验：512维可能欠拟合，2048维容易过拟合，1024维性能最佳
  MOE_EXPERT_HIDDEN_DIM: 1024   # 🏆 MoE专家网络隐藏层维度（经验最优：1024维）
  
  # 🏆 门控网络温度参数：0.7（经验最优值）
  # 原理：0.7使权重分布既不过于尖锐也不过于均匀
  # 经验：0.3太尖锐，1.0太均匀，0.7专家分工最明确
  MOE_TEMPERATURE: 0.7          # 🏆 MoE门控网络温度参数（经验最优：0.7）
  
  # 🏆 专家网络Dropout：0.1（经验最优值）
  # 原理：0.1提供适度的正则化，防止过拟合
  # 经验：0.0容易过拟合，0.2可能欠拟合，0.1最平衡
  MOE_EXPERT_DROPOUT: 0.1       # 🏆 专家网络Dropout比例（经验最优：0.1）
  
  # 🏆 门控网络Dropout：0.1（经验最优值）
  # 原理：0.1提升门控决策的稳定性
  # 经验：0.0门控不稳定，0.2决策过于保守，0.1最稳定
  MOE_GATE_DROPOUT: 0.1         # 🏆 门控网络Dropout比例（经验最优：0.1）
  
  # 🏆 专家网络层数：2层（经验最优值）
  # 原理：2层在表达能力和训练稳定性之间达到最佳平衡
  # 经验：1层可能欠拟合，3层容易过拟合，2层性能最佳
  MOE_EXPERT_LAYERS: 2          # 🏆 专家网络层数（经验最优：2层）
  
  # 🏆 门控网络层数：2层（经验最优值）
  # 原理：2层提供足够的决策复杂度，但不会过拟合
  # 经验：1层决策过于简单，3层容易过拟合，2层最稳定
  MOE_GATE_LAYERS: 2            # 🏆 门控网络层数（经验最优：2层）
  
  # 🏆 专家激活阈值：0.1（经验最优值）
  # 原理：0.1提供适度的激活敏感性，避免过于极端
  # 经验：0.0过于敏感，0.2过于保守，0.1最平衡
  MOE_EXPERT_THRESHOLD: 0.1     # 🏆 专家激活阈值（经验最优：0.1）
  
  # 🏆 残差连接权重：1.0（经验最优值）
  # 原理：1.0保持完整的残差连接，确保梯度流动
  # 经验：0.5梯度流动不足，1.0梯度流动最佳
  MOE_RESIDUAL_WEIGHT: 1.0      # 🏆 残差连接权重（经验最优：1.0）

INPUT:
  SIZE_TRAIN: [ 256, 128 ] # 训练尺寸
  SIZE_TEST: [ 256, 128 ] # 测试尺寸
  PROB: 0.5 # random horizontal flip # 随机水平翻转
  RE_PROB: 0.5 # random erasing # 随机擦除
  PADDING: 10 # 填充

DATALOADER:
  SAMPLER: 'softmax_triplet' # 采样器
  NUM_INSTANCE: 8 # 每个批次实例数 (提高采样多样性)
  NUM_WORKERS: 8 # 工作线程数

DATASETS:
  NAMES: ('RGBNT201') # 数据集名称
  ROOT_DIR: '/home/zubuntu/workspace/yzy/MambaPro/data/' # 数据集根目录

SOLVER:
  # 优化器设置
  OPTIMIZER_NAME: 'Adam' # 优化器名称
  BASE_LR: 0.0005 # 基础学习率 (论文baseline推荐值)
  WEIGHT_DECAY: 0.0005 # 权重衰减
  WEIGHT_DECAY_BIAS: 0.0005 # 偏置权重衰减
  MOMENTUM: 0.9 # 动量
  
  # 训练设置
  MAX_EPOCHS: 60 # 最大迭代数 (论文标准设置)
  IMS_PER_BATCH: 32 # 每个批次图像数 (提高训练稳定性)
  WARMUP_ITERS: 20 # 预热迭代数 (充分预热)
  WARMUP_METHOD: 'linear' # 预热方法
  WARMUP_FACTOR: 0.01 # 预热因子
  
  # 损失函数设置
  MARGIN: 0.3 # 三元组损失边界
  CENTER_LOSS_WEIGHT: 0.0005 # 中心损失权重
  CENTER_LR: 0.5 # 中心学习率
  
  # ========== 最优MoE损失权重：基于经验的精心调优 ==========
  # 🏆 专家平衡损失权重：0.01（经验最优值）
  # 原理：0.01促进专家使用平衡，但不过度约束
  # 经验：0.0专家不平衡，0.1过度约束，0.01最平衡
  MOE_BALANCE_LOSS_WEIGHT: 0.01 # 🏆 MoE专家平衡损失权重（经验最优：0.01）
  
  # 🏆 稀疏性损失权重：0.001（经验最优值）
  # 原理：0.001促进专家选择稀疏性，但不过度稀疏
  # 经验：0.0选择不稀疏，0.01过度稀疏，0.001最平衡
  MOE_SPARSITY_LOSS_WEIGHT: 0.001 # 🏆 MoE稀疏性损失权重（经验最优：0.001）
  
  # 🏆 多样性损失权重：0.01（经验最优值）
  # 原理：0.01促进专家分工多样性，但不过度约束
  # 经验：0.0分工不明确，0.1过度约束，0.01最平衡
  MOE_DIVERSITY_LOSS_WEIGHT: 0.01 # 🏆 MoE多样性损失权重（经验最优：0.01）
  
  # 学习率调度
  GAMMA: 0.1 # 衰减率
  STEPS: (30, 50) # 学习率衰减步数
  
  # 其他设置
  SEED: 42 # 随机种子 (确保可复现性)
  CHECKPOINT_PERIOD: 10 # 检查点保存周期
  LOG_PERIOD: 10 # 日志打印周期
  EVAL_PERIOD: 5 # 验证周期

TEST:
  IMS_PER_BATCH: 64 # 每个批次图像数 (测试时可以更大)
  RE_RANKING: 'no' # 是否重新排序
  WEIGHT: '/home/zubuntu/workspace/yzy/MambaPro/pths/MambaProbest.pth' # 权重路径
  NECK_FEAT: 'after' # 颈部特征 (使用after获得更好性能)
  FEAT_NORM: 'yes' # 特征归一化
  MISS: "nothing" # 缺失

# ========== 最优MoE实验输出目录 ==========
# 实验目的：为最优MoE实验设置专门的输出目录
# 功能：便于对比最优MoE与基线方法的性能差异
# 🏆 最优MoE实验：多尺度滑动窗口 + 最优MoE融合
OUTPUT_DIR: '/home/zubuntu/workspace/yzy/MambaPro/outputs/moe_optimal_experiment' # 最优MoE实验输出目录

# ========== 最优MoE实验使用说明 ==========
# 实验目的：验证最优MoE模块的有效性
# 
# 🏆 实验配置：
# - 多尺度滑动窗口：启用4×4、8×8、16×16三个尺度
# - MoE融合：启用专家网络动态融合
# - 最优参数：基于经验的精心调优参数组合
# 
# 🏆 执行命令：
# python train_net.py --config_file configs/RGBNT201/MambaPro_moe_optimal.yml
# 
# 🏆 最优参数说明：
# - 专家网络隐藏层：1024维（容量与效率最佳平衡）
# - 门控网络温度：0.7（权重分布最合理）
# - 专家网络层数：2层（表达能力与稳定性最佳平衡）
# - 门控网络层数：2层（决策复杂度与稳定性最佳平衡）
# - 专家激活阈值：0.1（激活敏感性最平衡）
# - 残差连接权重：1.0（梯度流动最佳）
# - 专家Dropout：0.1（正则化最平衡）
# - 门控Dropout：0.1（决策稳定性最佳）
# - 平衡损失权重：0.01（专家使用最平衡）
# - 稀疏性损失权重：0.001（专家选择最稀疏）
# - 多样性损失权重：0.01（专家分工最多样）
# 
# 🏆 预期效果：
# - 多尺度特征提取：捕获局部细节、结构信息、全局上下文
# - MoE动态融合：根据输入自适应选择重要尺度
# - 性能提升：相比基线方法显著提升mAP和Rank-1
# - 专家分工：每个专家专门处理特定尺度特征
# - 训练稳定：损失曲线平滑下降，收敛稳定
# 
# 🏆 分析维度：
# - 性能对比：与基线方法对比mAP和Rank-1提升
# - 专家分析：专家权重分布和激活模式
# - 特征分析：多尺度特征的互补性
# - 效率分析：计算开销与性能提升的权衡
# - 稳定性分析：训练过程的稳定性和收敛性
