"""
å¤šå°ºåº¦Mixture-of-Experts (MoE) ç‰¹å¾èåˆæ¨¡å—

åŠŸèƒ½ï¼š
- åœ¨ç°æœ‰å¤šå°ºåº¦æ»‘åŠ¨çª—å£åŸºç¡€ä¸Šï¼Œæ·»åŠ MoEä¸“å®¶ç½‘ç»œæœºåˆ¶
- é€šè¿‡é—¨æ§ç½‘ç»œåŠ¨æ€è®¡ç®—ä¸“å®¶æƒé‡
- å®ç°ä¸“ä¸šåŒ–å¤„ç†ä¸åŒå°ºåº¦ç‰¹å¾

ä½œè€…ï¼šåŸºäºidea-01.pngè®¾è®¡
æ—¥æœŸï¼š2024
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import math


class ExpertNetwork(nn.Module):
    """
    ğŸ”¥ ä¸“å®¶ç½‘ç»œæ¨¡å—
    
    æ¯ä¸ªä¸“å®¶ä¸“é—¨å¤„ç†ç‰¹å®šå°ºåº¦çš„ç‰¹å¾ï¼Œå®ç°ä¸“ä¸šåŒ–åˆ†å·¥
    """
    
    def __init__(self, input_dim=512, hidden_dim=1024, output_dim=512, dropout=0.1, num_layers=2):
        """
        åˆå§‹åŒ–ä¸“å®¶ç½‘ç»œ
        
        Args:
            input_dim (int): è¾“å…¥ç‰¹å¾ç»´åº¦
            hidden_dim (int): éšè—å±‚ç»´åº¦
            output_dim (int): è¾“å‡ºç‰¹å¾ç»´åº¦
            dropout (float): Dropoutæ¯”ä¾‹
            num_layers (int): ç½‘ç»œå±‚æ•°
        """
        super(ExpertNetwork, self).__init__()
        
        # ========== å¯é…ç½®å±‚æ•°çš„MLPä¸“å®¶ç½‘ç»œï¼šç‰¹å¾å¢å¼ºå¤„ç†å™¨ ==========
        # ğŸ”¥ åŠŸèƒ½ï¼šå¯¹å•ä¸ªå°ºåº¦çš„ç‰¹å¾è¿›è¡Œå¢å¼ºå¤„ç†ï¼Œæå‡è¡¨è¾¾èƒ½åŠ›
        # ğŸ¯ ä½œç”¨ï¼šç‰¹å¾å¢å¼º - è®©æ¯ä¸ªå°ºåº¦çš„ç‰¹å¾å˜å¾—æ›´"èªæ˜"
        # ğŸ“Š è¾“å…¥ï¼šinput_dim (512ç»´ï¼Œå•ä¸ªå°ºåº¦ç‰¹å¾)
        # ğŸ“Š è¾“å‡ºï¼šoutput_dim (512ç»´ï¼Œå¢å¼ºåçš„å°ºåº¦ç‰¹å¾)
        # ğŸ”§ å®ç°ï¼šå¯é…ç½®å±‚æ•°çš„MLP + LayerNorm + GELUæ¿€æ´» + Dropout + æ®‹å·®è¿æ¥
        
        layers = []
        current_dim = input_dim
        
        # æ„å»ºéšè—å±‚
        for i in range(num_layers - 1):
            layers.extend([
                nn.Linear(current_dim, hidden_dim),
                nn.LayerNorm(hidden_dim),
                nn.GELU(),
                nn.Dropout(dropout)
            ])
            current_dim = hidden_dim
        
        # è¾“å‡ºå±‚
        layers.extend([
            nn.Linear(current_dim, output_dim),
            nn.LayerNorm(output_dim),
            nn.GELU(),
            nn.Dropout(dropout)
        ])
        
        self.expert = nn.Sequential(*layers)
        
        # æ®‹å·®è¿æ¥çš„æŠ•å½±å±‚ï¼ˆå¦‚æœè¾“å…¥è¾“å‡ºç»´åº¦ä¸åŒï¼‰
        self.residual_proj = nn.Linear(input_dim, output_dim) if input_dim != output_dim else nn.Identity()
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        """
        ä¸“å®¶ç½‘ç»œå‰å‘ä¼ æ’­
        
        Args:
            x: [B, D] - è¾“å…¥ç‰¹å¾
        Returns:
            output: [B, D] - ä¸“å®¶å¤„ç†åçš„ç‰¹å¾
        """
        # ğŸ”¥ ä¸“å®¶ç½‘ç»œå¤„ç†æç¤ºï¼ˆä»…åœ¨ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶æ˜¾ç¤ºï¼‰
        if not hasattr(self, '_expert_forward_called'):
            print(f"ğŸ§  ä¸“å®¶ç½‘ç»œå¼€å§‹å¤„ç†ç‰¹å¾: {x.shape}")
            self._expert_forward_called = True
        
        # ========== MLPä¸“å®¶ç½‘ç»œå‰å‘ä¼ æ’­ï¼šç‰¹å¾å¢å¼ºå¤„ç† ==========
        # ğŸ”¥ åŠŸèƒ½ï¼šé€šè¿‡ä¸“å®¶ç½‘ç»œMLPå¯¹è¾“å…¥ç‰¹å¾è¿›è¡Œå¢å¼ºå¤„ç†
        # ğŸ¯ ä½œç”¨ï¼šç‰¹å¾å¢å¼º - è®©æ¯ä¸ªå°ºåº¦çš„ç‰¹å¾å˜å¾—æ›´"èªæ˜"
        # ğŸ“Š è¾“å…¥ï¼šx [B, 512] (å•ä¸ªå°ºåº¦ç‰¹å¾)
        # ğŸ“Š è¾“å‡ºï¼šoutput [B, 512] (å¢å¼ºåçš„å°ºåº¦ç‰¹å¾)
        expert_output = self.expert(x)  # MLPä¸“å®¶ç½‘ç»œå¤„ç†
        
        # æ®‹å·®è¿æ¥ï¼šä¿æŒåŸå§‹ä¿¡æ¯ï¼Œå¢å¼ºæ¢¯åº¦æµåŠ¨
        residual = self.residual_proj(x)
        output = expert_output + residual
        
        return output


class GatingNetwork(nn.Module):
    """
    ğŸ”¥ é—¨æ§ç½‘ç»œæ¨¡å—
    
    æ ¹æ®è¾“å…¥ç‰¹å¾åŠ¨æ€è®¡ç®—å„ä¸“å®¶çš„æƒé‡åˆ†å¸ƒ
    """
    
    def __init__(self, input_dim=1536, num_experts=3, temperature=1.0, dropout=0.1, num_layers=2):
        """
        åˆå§‹åŒ–é—¨æ§ç½‘ç»œ
        
        Args:
            input_dim (int): è¾“å…¥ç‰¹å¾ç»´åº¦ï¼ˆå¤šå°ºåº¦ç‰¹å¾æ‹¼æ¥åçš„ç»´åº¦ï¼‰
            num_experts (int): ä¸“å®¶æ•°é‡
            temperature (float): æ¸©åº¦å‚æ•°ï¼Œæ§åˆ¶æƒé‡åˆ†å¸ƒçš„å°–é”ç¨‹åº¦
            dropout (float): Dropoutæ¯”ä¾‹
            num_layers (int): ç½‘ç»œå±‚æ•°
        """
        super(GatingNetwork, self).__init__()
        self.num_experts = num_experts
        self.temperature = temperature
        
        # ========== å¯é…ç½®å±‚æ•°çš„MLPé—¨æ§ç½‘ç»œï¼šä¸“å®¶æƒé‡å†³ç­–å™¨ ==========
        # ğŸ”¥ åŠŸèƒ½ï¼šæ ¹æ®å¤šå°ºåº¦ç‰¹å¾è®¡ç®—å„ä¸“å®¶çš„æƒé‡åˆ†å¸ƒ
        # ğŸ¯ ä½œç”¨ï¼šæƒé‡è®¡ç®— - åˆ¤æ–­å“ªä¸ªå°ºåº¦çš„ç‰¹å¾æ›´é‡è¦
        # ğŸ“Š è¾“å…¥ï¼šinput_dim (1536ç»´ï¼Œ3ä¸ªå°ºåº¦ç‰¹å¾æ‹¼æ¥)
        # ğŸ“Š è¾“å‡ºï¼šnum_experts (3ç»´ï¼Œæ¯ä¸ªä¸“å®¶çš„æƒé‡)
        # ğŸ”§ å®ç°ï¼šå¯é…ç½®å±‚æ•°çš„MLP + LayerNorm + GELUæ¿€æ´» + Dropout
        
        layers = []
        current_dim = input_dim
        
        # æ„å»ºéšè—å±‚
        for i in range(num_layers - 1):
            next_dim = current_dim // 2 if i == 0 else current_dim
            layers.extend([
                nn.Linear(current_dim, next_dim),
                nn.LayerNorm(next_dim),
                nn.GELU(),
                nn.Dropout(dropout)
            ])
            current_dim = next_dim
        
        # è¾“å‡ºå±‚
        layers.append(nn.Linear(current_dim, num_experts))
        
        self.gate = nn.Sequential(*layers)
        
        # åˆå§‹åŒ–æƒé‡
        self._init_weights()
    
    def _init_weights(self):
        """åˆå§‹åŒ–æƒé‡"""
        for m in self.modules():
            if isinstance(m, nn.Linear):
                nn.init.xavier_uniform_(m.weight)
                if m.bias is not None:
                    nn.init.constant_(m.bias, 0)
    
    def forward(self, x):
        """
        é—¨æ§ç½‘ç»œå‰å‘ä¼ æ’­
        
        Args:
            x: [B, input_dim] - å¤šå°ºåº¦ç‰¹å¾æ‹¼æ¥
        Returns:
            weights: [B, num_experts] - ä¸“å®¶æƒé‡åˆ†å¸ƒ
        """
        # ğŸ”¥ é—¨æ§ç½‘ç»œå¤„ç†æç¤ºï¼ˆä»…åœ¨ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶æ˜¾ç¤ºï¼‰
        if not hasattr(self, '_gate_forward_called'):
            print(f"ğŸ¯ é—¨æ§ç½‘ç»œå¼€å§‹è®¡ç®—ä¸“å®¶æƒé‡: è¾“å…¥{x.shape} â†’ è¾“å‡º[{x.shape[0]}, {self.num_experts}]")
            self._gate_forward_called = True
        
        # ========== MLPé—¨æ§ç½‘ç»œå‰å‘ä¼ æ’­ï¼šè®¡ç®—ä¸“å®¶æƒé‡ ==========
        # ğŸ”¥ åŠŸèƒ½ï¼šé€šè¿‡é—¨æ§ç½‘ç»œMLPè®¡ç®—å„ä¸“å®¶çš„æƒé‡åˆ†å¸ƒ
        # ğŸ¯ ä½œç”¨ï¼šæƒé‡è®¡ç®— - åˆ¤æ–­å“ªä¸ªå°ºåº¦çš„ç‰¹å¾æ›´é‡è¦
        # ğŸ“Š è¾“å…¥ï¼šx [B, 1536] (å¤šå°ºåº¦ç‰¹å¾æ‹¼æ¥)
        # ğŸ“Š è¾“å‡ºï¼šweights [B, 3] (æ¯ä¸ªä¸“å®¶çš„æƒé‡)
        gate_scores = self.gate(x)  # [B, num_experts] - é—¨æ§ç½‘ç»œMLPå¤„ç†
        
        # åº”ç”¨æ¸©åº¦å‚æ•°ï¼šæ§åˆ¶æƒé‡åˆ†å¸ƒçš„å°–é”ç¨‹åº¦
        gate_scores = gate_scores / self.temperature
        
        # Softmaxå½’ä¸€åŒ–å¾—åˆ°æƒé‡åˆ†å¸ƒ
        weights = F.softmax(gate_scores, dim=-1)  # [B, num_experts]
        
        return weights


class MultiScaleMoE(nn.Module):
    """
    ğŸ”¥ å¤šå°ºåº¦Mixture-of-Expertsæ¨¡å—
    
    æ ¸å¿ƒåŠŸèƒ½ï¼š
    - æ¥æ”¶å¤šå°ºåº¦ç‰¹å¾ï¼ˆ4x4, 8x8, 16x16ï¼‰
    - é€šè¿‡é—¨æ§ç½‘ç»œè®¡ç®—ä¸“å®¶æƒé‡
    - ä½¿ç”¨ä¸“å®¶ç½‘ç»œå¤„ç†å¯¹åº”å°ºåº¦ç‰¹å¾
    - åŠ æƒèåˆå¾—åˆ°æœ€ç»ˆç‰¹å¾
    """
    
    def __init__(self, feat_dim=512, scales=[4, 8, 16], expert_hidden_dim=1024, temperature=1.0, 
                 expert_dropout=0.1, gate_dropout=0.1, expert_layers=2, gate_layers=2, 
                 expert_threshold=0.1, residual_weight=1.0):
        """
        åˆå§‹åŒ–å¤šå°ºåº¦MoEæ¨¡å—
        
        Args:
            feat_dim (int): ç‰¹å¾ç»´åº¦
            scales (list): æ»‘åŠ¨çª—å£å°ºåº¦åˆ—è¡¨
            expert_hidden_dim (int): ä¸“å®¶ç½‘ç»œéšè—å±‚ç»´åº¦
            temperature (float): é—¨æ§ç½‘ç»œæ¸©åº¦å‚æ•°
            expert_dropout (float): ä¸“å®¶ç½‘ç»œDropoutæ¯”ä¾‹
            gate_dropout (float): é—¨æ§ç½‘ç»œDropoutæ¯”ä¾‹
            expert_layers (int): ä¸“å®¶ç½‘ç»œå±‚æ•°
            gate_layers (int): é—¨æ§ç½‘ç»œå±‚æ•°
            expert_threshold (float): ä¸“å®¶æ¿€æ´»é˜ˆå€¼
            residual_weight (float): æ®‹å·®è¿æ¥æƒé‡
        """
        super(MultiScaleMoE, self).__init__()
        self.feat_dim = feat_dim
        self.scales = scales
        self.num_experts = len(scales)
        self.expert_threshold = expert_threshold
        self.residual_weight = residual_weight
        
        # ğŸ”¥ ä¸ºæ¯ä¸ªå°ºåº¦åˆ›å»ºä¸“é—¨çš„ä¸“å®¶ç½‘ç»œï¼ˆä½¿ç”¨é…ç½®å‚æ•°ï¼‰
        self.experts = nn.ModuleList()
        for i, scale in enumerate(scales):
            expert = ExpertNetwork(
                input_dim=feat_dim,
                hidden_dim=expert_hidden_dim,
                output_dim=feat_dim,
                dropout=expert_dropout,
                num_layers=expert_layers
            )
            self.experts.append(expert)
        
        # ğŸ”¥ é—¨æ§ç½‘ç»œï¼šæ ¹æ®å¤šå°ºåº¦ç‰¹å¾è®¡ç®—ä¸“å®¶æƒé‡ï¼ˆä½¿ç”¨é…ç½®å‚æ•°ï¼‰
        gate_input_dim = feat_dim * len(scales)  # 1536ç»´ï¼ˆ3ä¸ªå°ºåº¦Ã—512ç»´ï¼‰
        self.gating_network = GatingNetwork(
            input_dim=gate_input_dim,
            num_experts=self.num_experts,
            temperature=temperature,
            dropout=gate_dropout,
            num_layers=gate_layers
        )
        
        # ========== MLPæœ€ç»ˆèåˆå±‚ï¼šä¸“å®¶è¾“å‡ºèåˆå™¨ ==========
        # ğŸ”¥ åŠŸèƒ½ï¼šå°†MoEä¸“å®¶ç½‘ç»œçš„è¾“å‡ºè¿›è¡Œæœ€ç»ˆèåˆå¤„ç†
        # ğŸ¯ ä½œç”¨ï¼šç‰¹å¾èåˆ - å°†ä¸“å®¶è¾“å‡ºèåˆä¸ºå•ä¸€ç‰¹å¾
        # ğŸ“Š è¾“å…¥ï¼šfeat_dim (512ç»´ï¼ŒMoEåŠ æƒèåˆåçš„ç‰¹å¾)
        # ğŸ“Š è¾“å‡ºï¼šfeat_dim (512ç»´ï¼Œæœ€ç»ˆèåˆç‰¹å¾)
        # ğŸ”§ å®ç°ï¼šå•å±‚MLP + LayerNorm + GELUæ¿€æ´» + Dropout
        self.final_fusion = nn.Sequential(
            nn.Linear(feat_dim, feat_dim),  # MLPå±‚ï¼š512 -> 512 (ç‰¹å¾å¢å¼º)
            nn.LayerNorm(feat_dim),         # å±‚å½’ä¸€åŒ–ï¼šç¨³å®šè®­ç»ƒè¿‡ç¨‹
            nn.GELU(),                      # GELUæ¿€æ´»ï¼šå¢åŠ éçº¿æ€§è¡¨è¾¾èƒ½åŠ›
            nn.Dropout(0.1)                 # Dropoutæ­£åˆ™åŒ–ï¼šé˜²æ­¢è¿‡æ‹Ÿåˆ
        )
        
        print(f"ğŸ”¥ å¤šå°ºåº¦MoEæ¨¡å—åˆå§‹åŒ–å®Œæˆ:")
        print(f"   - ç‰¹å¾ç»´åº¦: {feat_dim}")
        print(f"   - æ»‘åŠ¨çª—å£å°ºåº¦: {scales}")
        print(f"   - ä¸“å®¶æ•°é‡: {self.num_experts}")
        print(f"   - é—¨æ§è¾“å…¥ç»´åº¦: {gate_input_dim}")
        print(f"   - ä¸“å®¶éšè—å±‚ç»´åº¦: {expert_hidden_dim}")
    
    def forward(self, multi_scale_features):
        """
        å¤šå°ºåº¦MoEå‰å‘ä¼ æ’­
        
        Args:
            multi_scale_features: List[Tensor] - å¤šå°ºåº¦ç‰¹å¾åˆ—è¡¨
                               æ¯ä¸ªå…ƒç´ å½¢çŠ¶ä¸º [B, feat_dim]
        Returns:
            final_feature: [B, feat_dim] - MoEèåˆåçš„æœ€ç»ˆç‰¹å¾
            expert_weights: [B, num_experts] - ä¸“å®¶æƒé‡åˆ†å¸ƒï¼ˆç”¨äºåˆ†æï¼‰
        """
        # ğŸ”¥ MoEæ¨¡å—å¯åŠ¨æç¤ºï¼ˆä»…åœ¨ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶æ˜¾ç¤ºï¼‰
        if not hasattr(self, '_moe_forward_called'):
            print(f"ğŸš€ å¤šå°ºåº¦MoEæ¨¡å—å¯åŠ¨ï¼")
            print(f"   - è¾“å…¥ç‰¹å¾æ•°é‡: {len(multi_scale_features)}")
            print(f"   - æ¯ä¸ªç‰¹å¾å½¢çŠ¶: {multi_scale_features[0].shape}")
            print(f"   - æ»‘åŠ¨çª—å£å°ºåº¦: {self.scales}")
            print(f"   - ä¸“å®¶æ•°é‡: {self.num_experts}")
            self._moe_forward_called = True
        
        B = multi_scale_features[0].shape[0]
        
        # ğŸ”¥ æ­¥éª¤1ï¼šæ‹¼æ¥å¤šå°ºåº¦ç‰¹å¾ä½œä¸ºé—¨æ§ç½‘ç»œè¾“å…¥
        concat_features = torch.cat(multi_scale_features, dim=1)  # [B, feat_dim * num_scales]
        
        # ========== MLPé—¨æ§ç½‘ç»œè°ƒç”¨ï¼šè®¡ç®—ä¸“å®¶æƒé‡ ==========
        # ğŸ”¥ åŠŸèƒ½ï¼šé€šè¿‡é—¨æ§ç½‘ç»œMLPè®¡ç®—å„ä¸“å®¶çš„æƒé‡åˆ†å¸ƒ
        # ğŸ¯ ä½œç”¨ï¼šæƒé‡è®¡ç®— - åˆ¤æ–­å“ªä¸ªå°ºåº¦çš„ç‰¹å¾æ›´é‡è¦
        # ğŸ“Š è¾“å…¥ï¼šconcat_features [B, 1536] (å¤šå°ºåº¦ç‰¹å¾æ‹¼æ¥)
        # ğŸ“Š è¾“å‡ºï¼šexpert_weights [B, 3] (æ¯ä¸ªä¸“å®¶çš„æƒé‡)
        expert_weights = self.gating_network(concat_features)  # [B, num_experts]
        
        # ========== MLPä¸“å®¶ç½‘ç»œè°ƒç”¨ï¼šå¤„ç†å„å°ºåº¦ç‰¹å¾ ==========
        # ğŸ”¥ åŠŸèƒ½ï¼šé€šè¿‡ä¸“å®¶ç½‘ç»œMLPå¤„ç†å„å°ºåº¦çš„ç‰¹å¾
        # ğŸ¯ ä½œç”¨ï¼šç‰¹å¾å¢å¼º - è®©æ¯ä¸ªå°ºåº¦çš„ç‰¹å¾å˜å¾—æ›´"èªæ˜"
        # ğŸ“Š è¾“å…¥ï¼šmulti_scale_features (List[Tensor], æ¯ä¸ªå…ƒç´ [B, 512])
        # ğŸ“Š è¾“å‡ºï¼šexpert_outputs (List[Tensor], æ¯ä¸ªå…ƒç´ [B, 512])
        expert_outputs = []
        for i, (expert, feature) in enumerate(zip(self.experts, multi_scale_features)):
            expert_output = expert(feature)  # [B, feat_dim] - ä¸“å®¶ç½‘ç»œMLPå¤„ç†
            expert_outputs.append(expert_output)
        
        # ğŸ”¥ æ­¥éª¤4ï¼šåŠ æƒèåˆä¸“å®¶è¾“å‡º
        # å°†ä¸“å®¶æƒé‡å¹¿æ’­åˆ°ç‰¹å¾ç»´åº¦
        weighted_outputs = []
        for i, expert_output in enumerate(expert_outputs):
            # expert_weights[:, i] å½¢çŠ¶ä¸º [B]ï¼Œéœ€è¦æ‰©å±•ä¸º [B, feat_dim]
            weight = expert_weights[:, i:i+1].expand_as(expert_output)  # [B, feat_dim]
            weighted_output = weight * expert_output  # [B, feat_dim]
            weighted_outputs.append(weighted_output)
        
        # æ±‚å’Œå¾—åˆ°èåˆç‰¹å¾
        fused_feature = torch.sum(torch.stack(weighted_outputs, dim=0), dim=0)  # [B, feat_dim]
        
        # ========== MLPæœ€ç»ˆèåˆå±‚è°ƒç”¨ï¼šä¸“å®¶è¾“å‡ºèåˆ ==========
        # ğŸ”¥ åŠŸèƒ½ï¼šé€šè¿‡æœ€ç»ˆèåˆå±‚MLPå¤„ç†MoEåŠ æƒèåˆåçš„ç‰¹å¾
        # ğŸ¯ ä½œç”¨ï¼šç‰¹å¾èåˆ - å°†ä¸“å®¶è¾“å‡ºèåˆä¸ºå•ä¸€ç‰¹å¾
        # ğŸ“Š è¾“å…¥ï¼šfused_feature [B, 512] (MoEåŠ æƒèåˆåçš„ç‰¹å¾)
        # ğŸ“Š è¾“å‡ºï¼šfinal_feature [B, 512] (æœ€ç»ˆèåˆç‰¹å¾)
        final_feature = self.final_fusion(fused_feature)  # [B, feat_dim]
        
        return final_feature, expert_weights
    
    def get_expert_usage_stats(self, expert_weights):
        """
        è·å–ä¸“å®¶ä½¿ç”¨ç»Ÿè®¡ä¿¡æ¯
        
        Args:
            expert_weights: [B, num_experts] - ä¸“å®¶æƒé‡åˆ†å¸ƒ
        Returns:
            stats: dict - ä¸“å®¶ä½¿ç”¨ç»Ÿè®¡
        """
        with torch.no_grad():
            # è®¡ç®—æ¯ä¸ªä¸“å®¶çš„å¹³å‡æƒé‡
            avg_weights = torch.mean(expert_weights, dim=0)  # [num_experts]
            
            # è®¡ç®—æ¯ä¸ªä¸“å®¶çš„æ¿€æ´»ç‡ï¼ˆæƒé‡>é˜ˆå€¼çš„æ¯”ä¾‹ï¼‰
            threshold = 0.1
            activation_rates = torch.mean((expert_weights > threshold).float(), dim=0)  # [num_experts]
            
            stats = {
                'avg_weights': avg_weights.cpu().numpy(),
                'activation_rates': activation_rates.cpu().numpy(),
                'scale_names': [f'{scale}x{scale}' for scale in self.scales]
            }
            
            return stats


class CLIPMultiScaleMoE(nn.Module):
    """
    ğŸ”¥ CLIPå…¼å®¹çš„å¤šå°ºåº¦MoEç‰¹å¾æå–å™¨
    
    é›†æˆå¤šå°ºåº¦æ»‘åŠ¨çª—å£å’ŒMoEæœºåˆ¶
    """
    
    def __init__(self, feat_dim=512, scales=[4, 8, 16], expert_hidden_dim=1024, temperature=1.0,
                 expert_dropout=0.1, gate_dropout=0.1, expert_layers=2, gate_layers=2, 
                 expert_threshold=0.1, residual_weight=1.0):
        """
        åˆå§‹åŒ–CLIPå¤šå°ºåº¦MoEæ¨¡å—
        
        Args:
            feat_dim (int): ç‰¹å¾ç»´åº¦
            scales (list): æ»‘åŠ¨çª—å£å°ºåº¦åˆ—è¡¨
            expert_hidden_dim (int): ä¸“å®¶ç½‘ç»œéšè—å±‚ç»´åº¦
            temperature (float): é—¨æ§ç½‘ç»œæ¸©åº¦å‚æ•°
            expert_dropout (float): ä¸“å®¶ç½‘ç»œDropoutæ¯”ä¾‹
            gate_dropout (float): é—¨æ§ç½‘ç»œDropoutæ¯”ä¾‹
            expert_layers (int): ä¸“å®¶ç½‘ç»œå±‚æ•°
            gate_layers (int): é—¨æ§ç½‘ç»œå±‚æ•°
            expert_threshold (float): ä¸“å®¶æ¿€æ´»é˜ˆå€¼
            residual_weight (float): æ®‹å·®è¿æ¥æƒé‡
        """
        super(CLIPMultiScaleMoE, self).__init__()
        self.feat_dim = feat_dim
        self.scales = scales
        
        # ğŸ”¥ å¤šå°ºåº¦æ»‘åŠ¨çª—å£å¤„ç†ï¼ˆå¤ç”¨ç°æœ‰å®ç°ï¼‰
        from .clip_multi_scale_sliding_window import CLIPMultiScaleSlidingWindow
        self.multi_scale_extractor = CLIPMultiScaleSlidingWindow(feat_dim, scales)
        
        # ğŸ”¥ MoEèåˆæ¨¡å—ï¼ˆä½¿ç”¨æ‰€æœ‰é…ç½®å‚æ•°ï¼‰
        self.moe_fusion = MultiScaleMoE(
            feat_dim=feat_dim,
            scales=scales,
            expert_hidden_dim=expert_hidden_dim,
            temperature=temperature,
            expert_dropout=expert_dropout,
            gate_dropout=gate_dropout,
            expert_layers=expert_layers,
            gate_layers=gate_layers,
            expert_threshold=expert_threshold,
            residual_weight=residual_weight
        )
        
        print(f"ğŸ”¥ CLIPå¤šå°ºåº¦MoEæ¨¡å—åˆå§‹åŒ–å®Œæˆ:")
        print(f"   - ç‰¹å¾ç»´åº¦: {feat_dim}")
        print(f"   - æ»‘åŠ¨çª—å£å°ºåº¦: {scales}")
        print(f"   - ä¸“å®¶éšè—å±‚ç»´åº¦: {expert_hidden_dim}")
    
    def forward(self, patch_tokens):
        """
        å‰å‘ä¼ æ’­
        
        Args:
            patch_tokens: [B, N, feat_dim] - CLIP patch tokens
        Returns:
            final_feature: [B, feat_dim] - MoEèåˆåçš„ç‰¹å¾
            expert_weights: [B, num_experts] - ä¸“å®¶æƒé‡åˆ†å¸ƒ
        """
        # ğŸ”¥ CLIPå¤šå°ºåº¦MoEå¯åŠ¨æç¤ºï¼ˆä»…åœ¨ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶æ˜¾ç¤ºï¼‰
        if not hasattr(self, '_clip_moe_forward_called'):
            print(f"ğŸ¯ CLIPå¤šå°ºåº¦MoEæ¨¡å—å¯åŠ¨ï¼")
            print(f"   - è¾“å…¥patch tokenså½¢çŠ¶: {patch_tokens.shape}")
            print(f"   - æ»‘åŠ¨çª—å£å°ºåº¦: {self.scales}")
            print(f"   - ç‰¹å¾ç»´åº¦: {self.feat_dim}")
            self._clip_moe_forward_called = True
        
        # ğŸ”¥ æ­¥éª¤1ï¼šå¤šå°ºåº¦æ»‘åŠ¨çª—å£ç‰¹å¾æå–
        # è¿™é‡Œéœ€è¦ä¿®æ”¹ç°æœ‰çš„å¤šå°ºåº¦æå–å™¨ï¼Œè¿”å›å„ä¸ªå°ºåº¦çš„ç‰¹å¾è€Œä¸æ˜¯èåˆåçš„ç‰¹å¾
        multi_scale_features = self._extract_multi_scale_features(patch_tokens)
        
        # ğŸ”¥ æ­¥éª¤2ï¼šMoEèåˆ
        final_feature, expert_weights = self.moe_fusion(multi_scale_features)
        
        return final_feature, expert_weights
    
    def _extract_multi_scale_features(self, patch_tokens):
        """
        æå–å¤šå°ºåº¦ç‰¹å¾ï¼ˆä¿®æ”¹ç°æœ‰å®ç°ä»¥è¿”å›å„å°ºåº¦ç‰¹å¾ï¼‰
        
        Args:
            patch_tokens: [B, N, feat_dim] - CLIP patch tokens
        Returns:
            multi_scale_features: List[Tensor] - å„å°ºåº¦ç‰¹å¾åˆ—è¡¨
        """
        B, N, D = patch_tokens.shape
        
        # è½¬æ¢ä¸ºå·ç§¯è¾“å…¥æ ¼å¼
        x = patch_tokens.transpose(1, 2)  # [B, feat_dim, N]
        
        multi_scale_features = []
        for i, scale in enumerate(self.scales):
            if N >= scale:
                # ä½¿ç”¨1Då·ç§¯è¿›è¡Œæ»‘åŠ¨çª—å£å¤„ç†
                windowed_feat = self.multi_scale_extractor.sliding_windows[i](x)  # [B, feat_dim, N//scale]
                # å…¨å±€å¹³å‡æ± åŒ–
                pooled_feat = F.adaptive_avg_pool1d(windowed_feat, 1).squeeze(-1)  # [B, feat_dim]
            else:
                # å¦‚æœåºåˆ—é•¿åº¦å°äºçª—å£å¤§å°ï¼Œç›´æ¥ä½¿ç”¨å…¨å±€å¹³å‡æ± åŒ–
                pooled_feat = F.adaptive_avg_pool1d(x, 1).squeeze(-1)  # [B, feat_dim]
            
            multi_scale_features.append(pooled_feat)
        
        return multi_scale_features


# æµ‹è¯•ä»£ç 
if __name__ == "__main__":
    print("=== å¤šå°ºåº¦MoEæ¨¡å—æµ‹è¯• ===")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size = 2
    seq_len = 196  # 14x14 patches
    feat_dim = 512
    
    # åˆ›å»ºæ¨¡å‹
    model = CLIPMultiScaleMoE(feat_dim=feat_dim, scales=[4, 8, 16])
    
    # åˆ›å»ºæµ‹è¯•è¾“å…¥
    patch_tokens = torch.randn(batch_size, seq_len, feat_dim)
    
    print(f"è¾“å…¥å½¢çŠ¶: {patch_tokens.shape}")
    
    # å‰å‘ä¼ æ’­
    with torch.no_grad():
        final_feature, expert_weights = model(patch_tokens)
    
    print(f"è¾“å‡ºç‰¹å¾å½¢çŠ¶: {final_feature.shape}")
    print(f"ä¸“å®¶æƒé‡å½¢çŠ¶: {expert_weights.shape}")
    print(f"ä¸“å®¶æƒé‡åˆ†å¸ƒ:")
    for i, scale in enumerate([4, 8, 16]):
        avg_weight = torch.mean(expert_weights[:, i]).item()
        print(f"  {scale}x{scale}çª—å£ä¸“å®¶: {avg_weight:.4f}")
    
    # è·å–ä¸“å®¶ä½¿ç”¨ç»Ÿè®¡
    stats = model.moe_fusion.get_expert_usage_stats(expert_weights)
    print(f"ä¸“å®¶æ¿€æ´»ç‡: {stats['activation_rates']}")
    
    print("âœ… å¤šå°ºåº¦MoEæ¨¡å—æµ‹è¯•é€šè¿‡ï¼")
