"""
MambaPro æ¨¡å‹æ„å»ºï¼ˆä¸­æ–‡è¯´æ˜ï¼‰

èŒè´£ï¼š
- å®šä¹‰è§†è§‰éª¨å¹²åŒ…è£…ç±» build_transformerï¼ˆæ”¯æŒ ViT/CLIP/T2T ç­‰ï¼‰
- å®šä¹‰æ•´ä½“æ¨¡å‹ MambaProï¼ˆå¤šæ¨¡æ€ RGB/NI/TI ç‰¹å¾æå–ä¸èåˆï¼Œæ”¯æŒ AAM/Mamba åˆ†æ”¯ï¼‰
- æä¾› make_model å·¥å‚å‡½æ•°æŒ‰é…ç½®å®ä¾‹åŒ–æ¨¡å‹

è¦ç‚¹ï¼š
- é€šè¿‡ cfg åˆ‡æ¢æ˜¯å¦ä½¿ç”¨ CLIPã€ç›¸æœº/è§†è§’åµŒå…¥ï¼ˆSIEï¼‰ã€LoRA å†»ç»“ç­‰
- è®­ç»ƒè¿”å›å¤šå¤´ logits/ç‰¹å¾ä»¥æ”¯æŒå¤šæŸå¤±ï¼›æµ‹è¯•è¿”å›æ‹¼æ¥æˆ–èåˆç‰¹å¾
"""
import torch
import torch.nn as nn
from modeling.backbones.vit_pytorch import vit_base_patch16_224, vit_small_patch16_224, \
    deit_small_patch16_224
from modeling.backbones.t2t import t2t_vit_t_14, t2t_vit_t_24
from timm.models.layers import trunc_normal_
from modeling.make_model_clipreid import load_clip_to_cpu
from modeling.clip.LoRA import mark_only_lora_as_trainable as lora_train
from modeling.fusion_part.AAM import AAM


def weights_init_kaiming(m):  # å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨ Kaiming åˆå§‹åŒ–æ–¹æ³•å¯¹æ¨¡å‹å±‚è¿›è¡Œåˆå§‹åŒ–
    classname = m.__class__.__name__  # è·å–å½“å‰å±‚çš„ç±»åï¼ˆå¦‚ 'Linear'ã€'Conv2d'ã€'BatchNorm2d' ç­‰ï¼‰
    
    if classname.find('Linear') != -1:  # å¦‚æœæ˜¯å…¨è¿æ¥å±‚ï¼ˆLinearï¼‰
        nn.init.kaiming_normal_(m.weight, a=0, mode='fan_out')  # ä½¿ç”¨ Kaiming æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–æƒé‡ï¼Œé€‚åˆ ReLU æ¿€æ´»
        nn.init.constant_(m.bias, 0.0)  # å°†åç½®åˆå§‹åŒ–ä¸º 0

    elif classname.find('Conv') != -1:  # å¦‚æœæ˜¯å·ç§¯å±‚ï¼ˆConvï¼‰
        nn.init.kaiming_normal_(m.weight, a=0, mode='fan_in')  # ä½¿ç”¨ Kaiming æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–å·ç§¯æ ¸æƒé‡
        if m.bias is not None:  # å¦‚æœå·ç§¯å±‚æœ‰åç½®é¡¹
            nn.init.constant_(m.bias, 0.0)  # å°†åç½®åˆå§‹åŒ–ä¸º 0

    elif classname.find('BatchNorm') != -1:  # å¦‚æœæ˜¯æ‰¹å½’ä¸€åŒ–å±‚ï¼ˆBatchNormï¼‰
        if m.affine:  # å¦‚æœ BatchNorm å±‚æœ‰å¯å­¦ä¹ å‚æ•°ï¼ˆweight å’Œ biasï¼‰
            nn.init.constant_(m.weight, 1.0)  # å°†ç¼©æ”¾å› å­ gamma åˆå§‹åŒ–ä¸º 1
            nn.init.constant_(m.bias, 0.0)   # å°†å¹³ç§»å› å­ beta åˆå§‹åŒ–ä¸º 0


def weights_init_classifier(m):  # å®šä¹‰ä¸€ä¸ªå‡½æ•°ï¼Œç”¨äºåˆå§‹åŒ–åˆ†ç±»å™¨å±‚ï¼ˆé€šå¸¸æ˜¯æœ€åä¸€å±‚ Linearï¼‰
    classname = m.__class__.__name__  # è·å–å±‚çš„ç±»å
    
    if classname.find('Linear') != -1:  # å¦‚æœæ˜¯å…¨è¿æ¥å±‚
        nn.init.normal_(m.weight, std=0.001)  # ä½¿ç”¨å‡å€¼ä¸º 0ï¼Œæ ‡å‡†å·®ä¸º 0.001 çš„æ­£æ€åˆ†å¸ƒåˆå§‹åŒ–æƒé‡
        if m.bias:  # å¦‚æœå­˜åœ¨åç½®
            nn.init.constant_(m.bias, 0.0)  # å°†åç½®åˆå§‹åŒ–ä¸º 0



class build_transformer(nn.Module):  # è§†è§‰éª¨å¹²å°è£…ï¼ˆå…¼å®¹ ViT/CLIP/T2T ç­‰ï¼‰
    def __init__(self, num_classes, cfg, camera_num, view_num, factory,feat_dim):
        super(build_transformer, self).__init__()
        model_path = cfg.MODEL.PRETRAIN_PATH_T  # é¢„è®­ç»ƒæƒé‡è·¯å¾„ï¼ˆImageNet/è‡ªå®šä¹‰ï¼‰
        self.in_planes = feat_dim  # ç‰¹å¾ç»´åº¦ï¼ˆçº¿æ€§åˆ†ç±»å™¨/BNNeckè¾“å…¥ï¼‰
        self.cv_embed_sign = cfg.MODEL.SIE_CAMERA  # æ˜¯å¦å¯ç”¨ç›¸æœº/è§†è§’åµŒå…¥
        self.neck = cfg.MODEL.NECK  # é¢ˆéƒ¨ç»“æ„ç±»å‹ï¼ˆå¦‚ bnneckï¼‰
        self.neck_feat = cfg.TEST.NECK_FEAT  # æµ‹è¯•é˜¶æ®µè¿”å› neck å‰/åç‰¹å¾
        self.model_name = cfg.MODEL.TRANSFORMER_TYPE  # éª¨å¹²ç±»å‹å
        self.trans_type = cfg.MODEL.TRANSFORMER_TYPE  # åŒä¸Š
        self.flops_test = cfg.MODEL.FLOPS_TEST  # FLOPs æµ‹è¯•æ ‡å¿—
        print('using Transformer_type: {} as a backbone'.format(cfg.MODEL.TRANSFORMER_TYPE))  # æ‰“å°éª¨å¹²ç±»å‹

        if cfg.MODEL.SIE_CAMERA:
            self.camera_num = camera_num  # ç›¸æœºæ•°é‡ï¼ˆç”¨äº SIEï¼‰
        else:
            self.camera_num = 0
        # No view
        self.view_num = 0  # è§†è§’æ•°æ­¤å¤„å›ºå®šä¸º0ï¼ˆå¦‚éœ€å¯æ‰©å±•ï¼‰
        
        # ğŸ”¥ æ–°å¢ï¼šCLIPå¤šå°ºåº¦æ»‘åŠ¨çª—å£é…ç½®
        # åŠŸèƒ½ï¼šä»é…ç½®æ–‡ä»¶è¯»å–CLIPå¤šå°ºåº¦æ»‘åŠ¨çª—å£è®¾ç½®
        # é»˜è®¤å€¼ï¼šFalseï¼ˆä¸å¯ç”¨å¤šå°ºåº¦å¤„ç†ï¼‰
        self.use_clip_multi_scale = getattr(cfg.MODEL, 'USE_CLIP_MULTI_SCALE', False)
        
        if cfg.MODEL.TRANSFORMER_TYPE == 'vit_base_patch16_224':
            # æ ‡å‡†ViTåˆ†æ”¯ï¼ˆä¿æŒåŸæœ‰åŠŸèƒ½ï¼‰
            self.base = factory[cfg.MODEL.TRANSFORMER_TYPE](img_size=cfg.INPUT.SIZE_TRAIN, sie_xishu=cfg.MODEL.SIE_COE,
                                                            num_classes=num_classes,
                                                            camera=self.camera_num, view=self.view_num,
                                                            stride_size=cfg.MODEL.STRIDE_SIZE,
                                                            drop_path_rate=cfg.MODEL.DROP_PATH,
                                                            drop_rate=cfg.MODEL.DROP_OUT,
                                                            attn_drop_rate=cfg.MODEL.ATT_DROP_RATE,
                                                            cfg = cfg)  # ä»å·¥å‚æ„å»º ViT
            self.clip = 0  # æ ‡è®°é CLIP åˆ†æ”¯
            self.base.load_param(model_path)  # åŠ è½½ ImageNet é¢„è®­ç»ƒ
            print('Loading pretrained model from ImageNet')  # æç¤ºä¿¡æ¯
            if cfg.MODEL.FROZEN:
                lora_train(self.base)  # ä»…è®­ç»ƒ LoRA å‚æ•°ï¼ˆå…¶ä½™å†»ç»“ï¼‰
        elif cfg.MODEL.TRANSFORMER_TYPE == 't2t_vit_t_24':
            # æ–°å¢ï¼šT2T-ViT-24æ¨¡å‹å¤„ç†
            # åŠŸèƒ½ï¼šåˆ›å»ºT2T-ViT-24æ¨¡å‹ï¼Œæ”¯æŒå¤šå°ºåº¦æ»‘åŠ¨çª—å£
            self.base = factory[cfg.MODEL.TRANSFORMER_TYPE](
                img_size=cfg.INPUT.SIZE_TRAIN,
                stride_size=cfg.MODEL.STRIDE_SIZE,
                drop_path_rate=cfg.MODEL.DROP_PATH,
                drop_rate=cfg.MODEL.DROP_RATE,
                attn_drop_rate=cfg.MODEL.ATT_DROP_RATE,
                camera=self.camera_num,
                view=self.view_num,
                sie_xishu=cfg.MODEL.SIE_COE,
                use_multi_scale=self.use_multi_scale  # ä¼ é€’å¤šå°ºåº¦å‚æ•°
            )
            self.clip = 0  # æ ‡è®°é CLIP åˆ†æ”¯
            self.base.load_param(model_path)  # åŠ è½½é¢„è®­ç»ƒæƒé‡
            print('Loading pretrained T2T-ViT-24 model')  # æç¤ºä¿¡æ¯
            if cfg.MODEL.FROZEN:
                lora_train(self.base)  # ä»…è®­ç»ƒ LoRA å‚æ•°ï¼ˆå…¶ä½™å†»ç»“ï¼‰
        elif cfg.MODEL.TRANSFORMER_TYPE == 'ViT-B-16':
            # æ¢å¤åŸä½œè€…çš„è®¾è®¡ï¼šViT-B-16èµ°CLIPåˆ†æ”¯
            # åŠŸèƒ½ï¼šä¿æŒåŸä½œè€…çš„CLIPå®ç°ï¼Œå¹¶æ·»åŠ å¤šå°ºåº¦æ»‘åŠ¨çª—å£æ”¯æŒ
            self.clip = 1  # æ ‡è®°èµ° CLIP åˆ†æ”¯
            self.sie_xishu = cfg.MODEL.SIE_COE  # SIE ç³»æ•°
            clip_model = load_clip_to_cpu(cfg, self.model_name, cfg.INPUT.SIZE_TRAIN[0] // cfg.MODEL.STRIDE_SIZE[0],
                                          cfg.INPUT.SIZE_TRAIN[1] // cfg.MODEL.STRIDE_SIZE[1],
                                          cfg.MODEL.STRIDE_SIZE)  # åŠ è½½ CLIP æ¨¡å‹
            print('Loading pretrained model from CLIP')  # æç¤ºä¿¡æ¯
            clip_model.to("cuda")  # å°† CLIP æ¨¡å‹ç§»è‡³ GPU
            self.base = clip_model.visual  # ä½¿ç”¨è§†è§‰ç¼–ç å™¨ä½œä¸ºéª¨å¹²
            if cfg.MODEL.FROZEN:
                lora_train(self.base)  # ä»…è®­ç»ƒ LoRA

            # ğŸ”¥ æ–°å¢ï¼šCLIPå¤šå°ºåº¦æ»‘åŠ¨çª—å£åˆå§‹åŒ–
            # åŠŸèƒ½ï¼šåœ¨CLIPåˆ†æ”¯åŸºç¡€ä¸Šæ·»åŠ å¤šå°ºåº¦æ»‘åŠ¨çª—å£ç‰¹å¾æå–
            if self.use_clip_multi_scale:
                from modeling.fusion_part.clip_multi_scale_sliding_window import CLIPMultiScaleFeatureExtractor
                # åˆå§‹åŒ–å¤šå°ºåº¦ç‰¹å¾æå–å™¨ï¼š512ç»´è¾“å…¥ï¼Œ4x4/8x8/16x16æ»‘åŠ¨çª—å£
                self.clip_multi_scale_extractor = CLIPMultiScaleFeatureExtractor(feat_dim=512, scales=[4, 8, 16])
                print('âœ… ä¸ºCLIPå¯ç”¨å¤šå°ºåº¦æ»‘åŠ¨çª—å£ç‰¹å¾æå–æ¨¡å—')
                print(f'   - æ»‘åŠ¨çª—å£å°ºåº¦: [4, 8, 16]')
                print(f'   - ç‰¹å¾ç»´åº¦: 512 (CLIPæŠ•å½±ç»´åº¦)')
            
            # ğŸ”¥ æ–°å¢ï¼šå¤šå°ºåº¦MoEé…ç½®å’Œåˆå§‹åŒ–
            # åŠŸèƒ½ï¼šä»é…ç½®æ–‡ä»¶è¯»å–MoEè®¾ç½®ï¼Œåˆå§‹åŒ–MoEæ¨¡å—
            self.use_multi_scale_moe = getattr(cfg.MODEL, 'USE_MULTI_SCALE_MOE', False)
            self.moe_scales = getattr(cfg.MODEL, 'MOE_SCALES', [4, 8, 16])
            
            if self.use_multi_scale_moe:
                from modeling.fusion_part.multi_scale_moe import CLIPMultiScaleMoE
                # åˆå§‹åŒ–å¤šå°ºåº¦MoEæ¨¡å—ï¼š512ç»´è¾“å…¥ï¼Œ4x4/8x8/16x16æ»‘åŠ¨çª—å£ï¼Œä¸“å®¶ç½‘ç»œ
                self.clip_multi_scale_moe = CLIPMultiScaleMoE(
                    feat_dim=512, 
                    scales=self.moe_scales,
                    expert_hidden_dim=1024,
                    temperature=1.0
                )
                # åˆå§‹åŒ–ä¸“å®¶æƒé‡å†å²è®°å½•ï¼ˆç”¨äºåˆ†æï¼‰
                self.expert_weights_history = []
                print('ğŸ”¥ ä¸ºCLIPå¯ç”¨å¤šå°ºåº¦MoEç‰¹å¾èåˆæ¨¡å—')
                print(f'   - æ»‘åŠ¨çª—å£å°ºåº¦: {self.moe_scales}')
                print(f'   - ç‰¹å¾ç»´åº¦: 512 (CLIPæŠ•å½±ç»´åº¦)')
                print(f'   - ä¸“å®¶éšè—å±‚ç»´åº¦: 1024')
                print(f'   - é—¨æ§ç½‘ç»œæ¸©åº¦: 1.0')

            if cfg.MODEL.SIE_CAMERA and cfg.MODEL.SIE_VIEW:
                self.cv_embed = nn.Parameter(torch.zeros(camera_num * view_num, 768))  # ç›¸æœºÃ—è§†è§’åµŒå…¥ï¼ˆCLIPå®é™…ç»´åº¦ï¼‰
                trunc_normal_(self.cv_embed, std=.02)  # æˆªæ–­æ­£æ€åˆå§‹åŒ–
                print('camera number is : {}'.format(camera_num))  # æ‰“å°ç›¸æœºæ•°
            elif cfg.MODEL.SIE_CAMERA:
                self.cv_embed = nn.Parameter(torch.zeros(camera_num, 768))  # ä»…ç›¸æœºåµŒå…¥ï¼ˆCLIPå®é™…ç»´åº¦ï¼‰
                trunc_normal_(self.cv_embed, std=.02)
                print('camera number is : {}'.format(camera_num))
            elif cfg.MODEL.SIE_VIEW:
                self.cv_embed = nn.Parameter(torch.zeros(view_num, 768))  # ä»…è§†è§’åµŒå…¥ï¼ˆCLIPå®é™…ç»´åº¦ï¼‰
                trunc_normal_(self.cv_embed, std=.02)
                print('camera number is : {}'.format(view_num))

        self.num_classes = num_classes
        self.ID_LOSS_TYPE = cfg.MODEL.ID_LOSS_TYPE

        self.classifier = nn.Linear(self.in_planes, self.num_classes, bias=False)  # çº¿æ€§åˆ†ç±»å¤´
        self.classifier.apply(weights_init_classifier)  # åˆ†ç±»å¤´åˆå§‹åŒ–

        self.bottleneck = nn.BatchNorm1d(self.in_planes)  # BNNeck
        self.bottleneck.bias.requires_grad_(False)  # å†»ç»“åç½®
        self.bottleneck.apply(weights_init_kaiming)  # BN åˆå§‹åŒ–

    def forward(self, x, label=None, cam_label=None, view_label=None, modality=None):
        if self.clip == 0:
            x = self.base(x, cam_label=cam_label, view_label=view_label, modality=modality)  # ViT/T2T å‰å‘
                
        else:
            # CLIPåˆ†æ”¯ - ä¿æŒåŸæœ‰é€»è¾‘
            if self.cv_embed_sign:
                if self.flops_test:
                    cam_label = 0  # FLOPs æµ‹è¯•æ—¶ç»Ÿä¸€ç›¸æœºç´¢å¼•
                cv_embed = self.sie_xishu * self.cv_embed[cam_label]  # å–ç›¸æœº/è§†è§’åµŒå…¥
            else:
                cv_embed = None  # ä¸ä½¿ç”¨åµŒå…¥
            x = self.base(x, cv_embed, modality)  # CLIP å‰å‘
            
            # ğŸ”¥ æ–°å¢ï¼šCLIPå¤šå°ºåº¦æ»‘åŠ¨çª—å£å¤„ç†
            # åŠŸèƒ½ï¼šåœ¨CLIPç‰¹å¾æå–åï¼Œæ·»åŠ å¤šå°ºåº¦æ»‘åŠ¨çª—å£å¤„ç†
            # å¤„ç†æµç¨‹ï¼šCLIPè¾“å‡º â†’ åˆ†ç¦»tokens â†’ å¤šå°ºåº¦å¤„ç† â†’ ç‰¹å¾èåˆ â†’ é‡æ–°ç»„åˆ
            if hasattr(self, 'use_clip_multi_scale') and self.use_clip_multi_scale and hasattr(self, 'clip_multi_scale_extractor'):
                # ğŸ”¥ åˆ†ç¦»CLS tokenå’Œpatch tokens
                # CLIPè¾“å‡ºæ ¼å¼ï¼š[CLS_token, patch_token1, patch_token2, ...]
                cls_token = x[:, 0:1, :]  # [B, 1, 512] - CLIPçš„CLS token
                patch_tokens = x[:, 1:, :]  # [B, N, 512] - CLIPçš„patch tokens
                
                # ğŸ”¥ æ£€æŸ¥æ˜¯å¦ä½¿ç”¨MoEèåˆ
                if hasattr(self, 'use_multi_scale_moe') and self.use_multi_scale_moe and hasattr(self, 'clip_multi_scale_moe'):
                    # ğŸ”¥ ä½¿ç”¨MoEèåˆå¤šå°ºåº¦ç‰¹å¾
                    # æ ¸å¿ƒç®—æ³•ï¼š4x4/8x8/16x16æ»‘åŠ¨çª—å£ â†’ MoEä¸“å®¶ç½‘ç»œ â†’ åŠ¨æ€æƒé‡èåˆ
                    multi_scale_feature, expert_weights = self.clip_multi_scale_moe(patch_tokens)  # [B, 512], [B, 3]
                    
                    # ğŸ”¥ MoEèåˆå®Œæˆæç¤ºï¼ˆä»…åœ¨ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶æ˜¾ç¤ºï¼‰
                    if not hasattr(self, '_moe_fusion_called'):
                        print(f"âœ… MoEå¤šå°ºåº¦ç‰¹å¾èåˆå®Œæˆï¼")
                        print(f"   - è¾“å‡ºç‰¹å¾å½¢çŠ¶: {multi_scale_feature.shape}")
                        print(f"   - ä¸“å®¶æƒé‡å½¢çŠ¶: {expert_weights.shape}")
                        print(f"   - ä¸“å®¶æƒé‡åˆ†å¸ƒ: {expert_weights[0].detach().cpu().numpy()}")
                        self._moe_fusion_called = True
                    
                    # ä¿å­˜ä¸“å®¶æƒé‡ç”¨äºåˆ†æï¼ˆå¯é€‰ï¼‰
                    if hasattr(self, 'expert_weights_history'):
                        self.expert_weights_history.append(expert_weights.detach().cpu())
                else:
                    # ğŸ”¥ ä½¿ç”¨ä¼ ç»ŸMLPèåˆå¤šå°ºåº¦ç‰¹å¾
                    # æ ¸å¿ƒç®—æ³•ï¼š4x4/8x8/16x16æ»‘åŠ¨çª—å£ â†’ MLPç‰¹å¾èåˆ
                    multi_scale_feature = self.clip_multi_scale_extractor(patch_tokens)  # [B, 512]
                    
                    # ğŸ”¥ æ»‘åŠ¨çª—å£èåˆå®Œæˆæç¤ºï¼ˆä»…åœ¨ç¬¬ä¸€æ¬¡è°ƒç”¨æ—¶æ˜¾ç¤ºï¼‰
                    if not hasattr(self, '_sliding_window_fusion_called'):
                        print(f"âœ… å¤šå°ºåº¦æ»‘åŠ¨çª—å£ç‰¹å¾èåˆå®Œæˆï¼")
                        print(f"   - è¾“å‡ºç‰¹å¾å½¢çŠ¶: {multi_scale_feature.shape}")
                        self._sliding_window_fusion_called = True
                
                # ğŸ”¥ å°†å¤šå°ºåº¦ç‰¹å¾ä¸CLS tokenç»“åˆï¼ˆæ®‹å·®è¿æ¥ï¼‰
                # å¢å¼ºCLS tokenï¼šåŸå§‹CLS + å¤šå°ºåº¦ç‰¹å¾
                enhanced_cls = cls_token + multi_scale_feature.unsqueeze(1)  # [B, 1, 512]
                
                # ğŸ”¥ é‡æ–°ç»„åˆtokensï¼šå¢å¼ºçš„CLS token + åŸå§‹patch tokens
                # ä¿æŒåŸå§‹åºåˆ—ç»“æ„ï¼Œä½†CLS tokenè¢«å¤šå°ºåº¦ç‰¹å¾å¢å¼º
                x = torch.cat([enhanced_cls, patch_tokens], dim=1)  # [B, N+1, 512]

        global_feat = x[:, 0]  # å–CLS token ä½œä¸ºå…¨å±€ç‰¹å¾
        feat = self.bottleneck(global_feat)  # è¿‡ BNNeckï¼ˆè®­ç»ƒå¸¸ç”¨ï¼‰

        if self.training:
            if self.ID_LOSS_TYPE in ('arcface', 'cosface', 'amsoftmax', 'circle'):
                cls_score = self.classifier(feat, label)  # ç‰¹æ®Š margin ç±»å¤´ï¼ˆéœ€è¦ labelï¼‰
            else:
                cls_score = self.classifier(feat)  # æ™®é€šçº¿æ€§åˆ†ç±»
            return x, cls_score, global_feat  # è¿”å›ç¼“å­˜ã€åˆ†ç±»åˆ†æ•°ã€å…¨å±€ç‰¹å¾
        else:
            if self.neck_feat == 'after':
                return x, feat  # æµ‹è¯•è¿”å› BN åç‰¹å¾
            else:
                return x, global_feat  # æµ‹è¯•è¿”å› BN å‰ç‰¹å¾

    def load_param(self, trained_path):  # ä»æƒé‡æ–‡ä»¶åŠ è½½å‚æ•°ï¼ˆå…¼å®¹DP/DDPå‰ç¼€ï¼‰
        param_dict = torch.load(trained_path)
        for i in param_dict:
            self.state_dict()[i.replace('module.', '')].copy_(param_dict[i])
        print('Loading pretrained model from {}'.format(trained_path))  # æ‰“å°æ¥æº

    def load_param_finetune(self, model_path):  # ç²¾è°ƒï¼šä¸¥æ ¼æŒ‰é”®æ‹·è´
        param_dict = torch.load(model_path)
        for i in param_dict:
            self.state_dict()[i].copy_(param_dict[i])
        print('Loading pretrained model for finetuning from {}'.format(model_path))


class MambaPro(nn.Module):  # ä¸‰æ¨¡æ€ç»„è£…ä¸èåˆ head
    def __init__(self, num_classes, cfg, camera_num, view_num, factory):
        super(MambaPro, self).__init__()
        if 'vit_base_patch16_224' in cfg.MODEL.TRANSFORMER_TYPE:
            self.feat_dim = 768  # ViT åŸºæœ¬ç»´åº¦
        elif 'ViT-B-16' in cfg.MODEL.TRANSFORMER_TYPE:
            self.feat_dim = 512  # CLIP ViT-B/16 ç»´åº¦
        self.BACKBONE = build_transformer(num_classes, cfg, camera_num, view_num, factory,feat_dim=self.feat_dim)  # å…±äº«éª¨å¹²
        self.num_classes = num_classes
        self.cfg = cfg
        self.num_instance = cfg.DATALOADER.NUM_INSTANCE  # æ¯IDæ ·æœ¬æ•°ï¼ˆé‡‡æ ·ç­–ç•¥ç”¨ï¼‰
        self.camera = camera_num  # ç›¸æœºæ•°
        self.view = view_num  # è§†è§’æ•°
        self.direct = cfg.MODEL.DIRECT  # æ˜¯å¦ç›´æ¥æ‹¼æ¥åˆ†ç±»
        self.neck = cfg.MODEL.NECK  # é¢ˆéƒ¨ç±»å‹
        self.neck_feat = cfg.TEST.NECK_FEAT  # æµ‹è¯•ç‰¹å¾é€‰æ‹©
        self.ID_LOSS_TYPE = cfg.MODEL.ID_LOSS_TYPE  # åˆ†ç±»å¤´ç±»å‹
        self.mamba = cfg.MODEL.MAMBA  # æ˜¯å¦å¯ç”¨ Mamba èåˆ
        
        # ä½¿ç”¨åŸå§‹AAMèåˆæ¨¡å—
        self.AAM = AAM(self.feat_dim, n_layers=2, cfg=cfg)
        self.miss_type = cfg.TEST.MISS  # æµ‹è¯•ç¼ºå¤±æ¨¡æ€ç­–ç•¥
        self.classifier = nn.Linear(3 * self.feat_dim, self.num_classes, bias=False)  # åŸå§‹ä¸‰æ¨¡æ€æ‹¼æ¥åˆ†ç±»å¤´
        self.classifier.apply(weights_init_classifier)
        self.bottleneck = nn.BatchNorm1d(3 * self.feat_dim)  # åŸå§‹æ‹¼æ¥ BNNeck
        self.bottleneck.bias.requires_grad_(False)
        self.bottleneck.apply(weights_init_kaiming)

        self.classifier_fuse = nn.Linear(3 * self.feat_dim, self.num_classes, bias=False)  # èåˆç‰¹å¾åˆ†ç±»å¤´
        self.classifier_fuse.apply(weights_init_classifier)
        self.bottleneck_fuse = nn.BatchNorm1d(3 * self.feat_dim)  # èåˆ BNNeck
        self.bottleneck_fuse.bias.requires_grad_(False)
        self.bottleneck_fuse.apply(weights_init_kaiming)

    def load_param(self, trained_path):  # ç²¾ç¡®åŠ è½½ï¼ˆä¸å»æ‰ module å‰ç¼€ï¼‰
        param_dict = torch.load(trained_path)
        for i in param_dict:
            self.state_dict()[i].copy_(param_dict[i])
        print('Loading pretrained model from {}'.format(trained_path))

    def forward(self, x, label=None, cam_label=None, view_label=None):  # è®­ç»ƒ/æµ‹è¯•ä¸¤æ¡è·¯å¾„
        if self.training:
            RGB = x['RGB']  # å¯è§å…‰
            NI = x['NI']  # è¿‘çº¢å¤–
            TI = x['TI']  # çƒ­çº¢å¤–

            RGB_cash, RGB_score, RGB_global = self.BACKBONE(RGB, cam_label=cam_label, view_label=view_label,
                                                            modality='rgb')
            NI_cash, NI_score, NI_global = self.BACKBONE(NI, cam_label=cam_label, view_label=view_label, modality='nir')
            TI_cash, TI_score, TI_global = self.BACKBONE(TI, cam_label=cam_label, view_label=view_label, modality='tir')

            ori = torch.cat([RGB_global, NI_global, TI_global], dim=-1)  # ä¸‰æ¨¡æ€æ‹¼æ¥
            ori_global = self.bottleneck(ori)  # BNNeck
            ori_score = self.classifier(ori_global)  # åŸå§‹æ‹¼æ¥åˆ†ç±»

            if self.mamba:
                fuse = self.AAM(RGB_cash, NI_cash, TI_cash)  # èåˆåºåˆ—ï¼ˆå¦‚ Mambaï¼‰
                fuse_global = self.bottleneck_fuse(fuse)  # BNNeck èåˆ
                fuse_score = self.classifier_fuse(fuse_global)  # èåˆåˆ†ç±»

            if self.direct:  # ç›´æ¥è¾“å‡ºæ‹¼æ¥/èåˆç”¨äºåˆ†ç±»ï¼ˆç®€åŒ– headsï¼‰
                if self.mamba:
                    return ori_score, ori, fuse_score, fuse  # åŸå§‹ä¸èåˆå¹¶è¡Œè¾“å‡º
                else:
                    return ori_score, ori 
            else:
                if self.mamba: 
                    return RGB_score, RGB_global, NI_score, NI_global, TI_score, TI_global, fuse_score, fuse  # å¤šå¤´å¤šå°ºåº¦æŸå¤±
                else:
                    return RGB_score, RGB_global, NI_score, NI_global, TI_score, TI_global

        else:
            RGB = x['RGB']  # æµ‹è¯•è·¯å¾„
            NI = x['NI']    
            TI = x['TI']
            RGB_cash, RGB_global = self.BACKBONE(RGB, cam_label=cam_label, view_label=view_label, modality='rgb')
            NI_cash, NI_global = self.BACKBONE(NI, cam_label=cam_label, view_label=view_label, modality='nir')
            TI_cash, TI_global = self.BACKBONE(TI, cam_label=cam_label, view_label=view_label, modality='tir')

            if self.mamba:
                fuse = self.AAM(RGB_cash, NI_cash, TI_cash)  # è¾“å‡ºèåˆç‰¹å¾
                return fuse
            else:
                ori = torch.cat([RGB_global, NI_global, TI_global], dim=-1)  # è¾“å‡ºæ‹¼æ¥ç‰¹å¾
                return ori

# ä½œç”¨ï¼šæŠŠäººç±»å¥½è®°çš„å­—ç¬¦ä¸²åå­—ï¼Œç¿»è¯‘æˆä»£ç é‡ŒçœŸæ­£å¯è°ƒç”¨çš„æ¨¡å‹æ„é€ å‡½æ•°
__factory_T_type = {  # éª¨å¹²å·¥å‚æ˜ å°„
    'vit_base_patch16_224': vit_base_patch16_224, 
    'deit_base_patch16_224': vit_base_patch16_224,
    'vit_small_patch16_224': vit_small_patch16_224,
    'deit_small_patch16_224': deit_small_patch16_224,
    't2t_vit_t_14': t2t_vit_t_14,
    't2t_vit_t_24': t2t_vit_t_24,
}


def make_model(cfg, num_class, camera_num, view_num=0):  # æ¨¡å‹å·¥å‚
    model = MambaPro(num_class, cfg, camera_num, view_num, __factory_T_type)  # å®ä¾‹åŒ– MambaPro
    print('===========Building MambaPro===========')  # æ„å»ºæç¤º
    return model  # è¿”å›æ¨¡å‹
