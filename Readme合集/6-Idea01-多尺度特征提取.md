# å¤šå°ºåº¦ç‰¹å¾æå–é›†æˆæŒ‡å¯¼

## ğŸ¯ é›†æˆç›®æ ‡

æ ¹æ®Reidpic.pngå›¾ç‰‡ä¸­çš„æµç¨‹ï¼Œåœ¨é¦–ä½œè€…ä»£ç çš„åŸºç¡€ä¸Šï¼Œå®ç°åŸºäºæ»‘åŠ¨çª—å£çš„å¤šå°ºåº¦ç‰¹å¾æå–åˆ›æ–°ç‚¹ã€‚

## ğŸ“ å½“å‰çŠ¶æ€åˆ†æ

### âœ… å·²å®Œæˆæ¸…ç†
- **MoEæ¨¡å—å·²åˆ é™¤**: å®Œå…¨ç§»é™¤äº†å¤šå°ºåº¦MoEç›¸å…³ä»£ç 
- **ä»£ç å·²æ¸…ç†**: æ¸…ç†äº†æ‰€æœ‰MoEç›¸å…³çš„å¯¼å…¥å’Œå¼•ç”¨
- **é…ç½®å·²ç®€åŒ–**: ç§»é™¤äº†MoEç›¸å…³é…ç½®

### ğŸ”§ éœ€è¦å®ç°éƒ¨åˆ†
- **æ»‘åŠ¨çª—å£æœºåˆ¶**: å®ç°å¤šå°ºåº¦æ»‘åŠ¨çª—å£ç‰¹å¾æå–
- **T2Té›†æˆ**: å°†æ»‘åŠ¨çª—å£é›†æˆåˆ°T2Tç‰¹å¾æå–æµç¨‹ä¸­
- **ç‰¹å¾èåˆ**: å®ç°å¤šå°ºåº¦ç‰¹å¾çš„èåˆæœºåˆ¶

## ğŸ–¼ï¸ åŸºäºå›¾ç‰‡æµç¨‹çš„é›†æˆæ–¹æ¡ˆ

### å›¾ç‰‡æµç¨‹åˆ†æ

æ ¹æ®Reidpic.pngçš„æµç¨‹ï¼Œå¤šå°ºåº¦ç‰¹å¾æå–åº”è¯¥æŒ‰ç…§ä»¥ä¸‹é¡ºåºè¿›è¡Œï¼š

```
è¾“å…¥æ¨¡æ€å›¾åƒ â†’ Tokenåºåˆ— â†’ å¤šå°ºåº¦æ»‘åŠ¨çª—å£ â†’ ç‰¹å¾èåˆ â†’ è¾“å‡º
```

### æ¨èæ–¹æ¡ˆï¼šåœ¨T2T_ViTä¸­é›†æˆæ»‘åŠ¨çª—å£

**é€‰æ‹©ç†ç”±**ï¼š
1. **æµç¨‹åŒ¹é…**: å›¾ç‰‡æ˜¾ç¤ºå¤šå°ºåº¦å¤„ç†åœ¨Tokenåºåˆ—åŸºç¡€ä¸Šè¿›è¡Œï¼Œæ­£å¥½å¯¹åº”T2T_ViTçš„è¾“å‡º
2. **æ¶æ„ä¸€è‡´**: ä¸ç°æœ‰çš„T2Tæœºåˆ¶å®Œç¾ç»“åˆ
3. **æ€§èƒ½ä¼˜åŒ–**: åœ¨ç‰¹å¾æå–æ—©æœŸè¿›è¡Œå¤šå°ºåº¦å¤„ç†ï¼Œå……åˆ†åˆ©ç”¨tokenåºåˆ—ç»“æ„

## ğŸ”§ å…·ä½“ä»£ç ä¿®æ”¹

### ä¿®æ”¹ä½ç½®1ï¼šåˆ›å»ºå¤šå°ºåº¦æ»‘åŠ¨çª—å£æ¨¡å—

**æ–‡ä»¶ä½ç½®**: æ–°å»º `modeling/fusion_part/multi_scale_sliding_window.py`

```python
"""
å¤šå°ºåº¦æ»‘åŠ¨çª—å£ç‰¹å¾æå–æ¨¡å—
åŸºäºReidpic.pngçš„åˆ›æ–°è®¾è®¡

åŠŸèƒ½ï¼š
1. å¤šå°ºåº¦æ»‘åŠ¨çª—å£ç‰¹å¾æå–ï¼ˆ4x4, 8x8, 16x16ï¼‰
2. ç‰¹å¾èåˆæœºåˆ¶
3. ä¸T2T-ViTçš„é›†æˆ
"""

import torch
import torch.nn as nn
import torch.nn.functional as F


class MultiScaleSlidingWindow(nn.Module):
    """
    å¤šå°ºåº¦æ»‘åŠ¨çª—å£ç‰¹å¾æå–
    
    åŠŸèƒ½ï¼š
    - ä½¿ç”¨ä¸åŒå¤§å°çš„æ»‘åŠ¨çª—å£ï¼ˆ4x4, 8x8, 16x16ï¼‰æå–å¤šå°ºåº¦ç‰¹å¾
    - å°†tokenåºåˆ—æŒ‰ä¸åŒå°ºåº¦åˆ†ç»„ï¼Œæ•è·å¤šå°ºåº¦ä¿¡æ¯
    - å®ç°ä»å±€éƒ¨ç»†èŠ‚åˆ°å…¨å±€ä¸Šä¸‹æ–‡çš„å…¨æ–¹ä½ç‰¹å¾æ•è·
    """
    
    def __init__(self, dim, scales=[4, 8, 16]):
        super().__init__()
        self.scales = scales  # æ»‘åŠ¨çª—å£å°ºåº¦åˆ—è¡¨ [4, 8, 16]
        self.dim = dim        # ç‰¹å¾ç»´åº¦
        
        # ä¸ºæ¯ä¸ªå°ºåº¦åˆ›å»ºä¸“é—¨çš„ç‰¹å¾æå–å™¨
        self.scale_extractors = nn.ModuleList([
            nn.Sequential(
                nn.Linear(dim, dim),    # çº¿æ€§å˜æ¢
                nn.LayerNorm(dim),      # å±‚å½’ä¸€åŒ–
                nn.GELU()               # GELUæ¿€æ´»å‡½æ•°
            ) for _ in scales  # ä¸ºæ¯ä¸ªå°ºåº¦åˆ›å»ºä¸€ä¸ªæå–å™¨
        ])
        
        # ç‰¹å¾èåˆç½‘ç»œ
        self.fusion = nn.Sequential(
            nn.Linear(dim * len(scales), dim),  # å¤šå°ºåº¦ç‰¹å¾èåˆ
            nn.LayerNorm(dim),
            nn.GELU()
        )
        
    def forward(self, x):
        """
        å¤šå°ºåº¦æ»‘åŠ¨çª—å£ç‰¹å¾æå–å‰å‘ä¼ æ’­
        
        å‚æ•°ï¼š
        - x: [B, N, D] - è¾“å…¥tokenåºåˆ— (batch_size, sequence_length, feature_dim)
        
        è¿”å›ï¼š
        - fused_features: [B, D] - èåˆåçš„å¤šå°ºåº¦ç‰¹å¾
        """
        B, N, D = x.shape
        scale_features = []  # å­˜å‚¨æ¯ä¸ªå°ºåº¦çš„ç‰¹å¾
        
        for i, scale in enumerate(self.scales):
            if N >= scale:
                # æ»‘åŠ¨çª—å£åˆ†ç»„ï¼šå°†åºåˆ—æŒ‰æŒ‡å®šå°ºåº¦åˆ†ç»„
                num_windows = N - scale + 1  # å¯ç”Ÿæˆçš„çª—å£æ•°é‡
                windows = []
                
                for j in range(num_windows):
                    # æå–ç¬¬jä¸ªçª—å£çš„tokenåºåˆ—
                    window_tokens = x[:, j:j+scale, :]  # [B, scale, D]
                    windows.append(window_tokens)
                
                # å †å æ‰€æœ‰çª—å£ï¼š[B, num_windows, scale, D]
                windows = torch.stack(windows, dim=1)
                # é‡å¡‘ä¸ºï¼š[B*num_windows, scale, D] ä¾¿äºæ‰¹é‡å¤„ç†
                windows = windows.view(B * num_windows, scale, D)
                
                # é€šè¿‡å¯¹åº”çš„ç‰¹å¾æå–å™¨å¤„ç†
                scale_feature = self.scale_extractors[i](windows)  # [B*num_windows, scale, D]
                
                # å…¨å±€å¹³å‡æ± åŒ–å¾—åˆ°å°ºåº¦ç‰¹å¾
                scale_feature = torch.mean(scale_feature, dim=1)  # [B*num_windows, D]
                scale_feature = scale_feature.view(B, num_windows, D)
                scale_feature = torch.mean(scale_feature, dim=1)  # [B, D] æœ€ç»ˆå°ºåº¦ç‰¹å¾
                
            else:
                # å¦‚æœåºåˆ—é•¿åº¦å°äºçª—å£å¤§å°ï¼Œç›´æ¥ä½¿ç”¨å…¨å±€ç‰¹å¾
                scale_feature = torch.mean(x, dim=1)  # [B, D] å…¨å±€å¹³å‡æ± åŒ–
                scale_feature = self.scale_extractors[i](scale_feature.unsqueeze(1)).squeeze(1)
            
            scale_features.append(scale_feature)  # æ·»åŠ åˆ°ç‰¹å¾åˆ—è¡¨
        
        # æ‹¼æ¥æ‰€æœ‰å°ºåº¦çš„ç‰¹å¾
        concatenated_features = torch.cat(scale_features, dim=-1)  # [B, D*num_scales]
        
        # é€šè¿‡èåˆç½‘ç»œå¾—åˆ°æœ€ç»ˆç‰¹å¾
        fused_features = self.fusion(concatenated_features)  # [B, D]
        
        return fused_features


class MultiScaleFeatureExtractor(nn.Module):
    """
    å¤šå°ºåº¦ç‰¹å¾æå–å™¨
    
    åŠŸèƒ½ï¼š
    - é›†æˆå¤šå°ºåº¦æ»‘åŠ¨çª—å£
    - æä¾›ä¸T2T-ViTçš„æ¥å£
    - å®ç°ç‰¹å¾å¢å¼ºæœºåˆ¶
    """
    
    def __init__(self, embed_dim, scales=[4, 8, 16]):
        super().__init__()
        self.embed_dim = embed_dim
        self.scales = scales
        
        # å¤šå°ºåº¦æ»‘åŠ¨çª—å£
        self.multi_scale_window = MultiScaleSlidingWindow(embed_dim, scales)
        
        # ç‰¹å¾å¢å¼ºç½‘ç»œ
        self.enhancement = nn.Sequential(
            nn.Linear(embed_dim, embed_dim),
            nn.LayerNorm(embed_dim),
            nn.GELU(),
            nn.Dropout(0.1)
        )
        
    def forward(self, x):
        """
        å¤šå°ºåº¦ç‰¹å¾æå–å‰å‘ä¼ æ’­
        
        å‚æ•°ï¼š
        - x: [B, N, D] - è¾“å…¥tokenåºåˆ—
        
        è¿”å›ï¼š
        - enhanced_features: [B, D] - å¢å¼ºåçš„å¤šå°ºåº¦ç‰¹å¾
        """
        # å¤šå°ºåº¦ç‰¹å¾æå–
        multi_scale_features = self.multi_scale_window(x)  # [B, D]
        
        # ç‰¹å¾å¢å¼º
        enhanced_features = self.enhancement(multi_scale_features)  # [B, D]
        
        return enhanced_features
```

### ä¿®æ”¹ä½ç½®2ï¼šT2T_ViTç±»çš„__init__æ–¹æ³•

**æ–‡ä»¶ä½ç½®**: `modeling/backbones/t2t.py`
**ä¿®æ”¹ä½ç½®**: `T2T_ViT.__init__` æ–¹æ³•

```python
def __init__(self, img_size=(256, 128), tokens_type='performer', in_chans=3, num_classes=1000, embed_dim=768,
             depth=12, num_heads=12, mlp_ratio=4., qkv_bias=False, qk_scale=None, drop_rate=0., 
             attn_drop_rate=0., drop_path_rate=0., camera=0, view=0, sie_xishu=3.0, 
             norm_layer=nn.LayerNorm, token_dim=64, use_multi_scale=False):  # æ–°å¢å‚æ•°
    super().__init__()
    self.num_classes = num_classes
    self.num_features = self.embed_dim = embed_dim

    # åˆå§‹åŒ–T2Tç¼–ç æ¨¡å—
    self.tokens_to_token = T2T_module(
        img_size=img_size, tokens_type=tokens_type, in_chans=in_chans, 
        embed_dim=embed_dim, token_dim=token_dim)
    num_patches = self.tokens_to_token.num_patches

    # åˆå§‹åŒ–åˆ†ç±»tokenå’Œä½ç½®ç¼–ç 
    self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))  # å¯å­¦ä¹ çš„åˆ†ç±»token
    self.pos_embed = nn.Parameter(data=get_sinusoid_encoding(n_position=num_patches + 1, d_hid=embed_dim),
                                  requires_grad=False)  # å›ºå®šçš„æ­£å¼¦ä½ç½®ç¼–ç 
    self.pos_drop = nn.Dropout(p=drop_rate)  # ä½ç½®ç¼–ç åçš„dropout
    
    # SIEï¼ˆSide Information Embeddingï¼‰ç›¸å…³å‚æ•°
    self.cam_num = camera  # ç›¸æœºæ•°é‡
    self.view_num = view   # è§†è§’æ•°é‡
    self.sie_xishu = sie_xishu  # SIEåµŒå…¥ç³»æ•°
    
    # åˆå§‹åŒ–SIEåµŒå…¥
    if camera > 1 and view > 1:  # åŒæ—¶ä½¿ç”¨ç›¸æœºå’Œè§†è§’åµŒå…¥
        self.sie_embed = nn.Parameter(torch.zeros(camera * view, 1, embed_dim))
        trunc_normal_(self.sie_embed, std=.02)
        print('camera number is : {} and viewpoint number is : {}'.format(camera, view))
        print('using SIE_Lambda is : {}'.format(sie_xishu))
    elif camera > 1:  # ä»…ä½¿ç”¨ç›¸æœºåµŒå…¥
        self.sie_embed = nn.Parameter(torch.zeros(camera, 1, embed_dim))
        trunc_normal_(self.sie_embed, std=.02)
        print('camera number is : {}'.format(camera))
        print('using SIE_Lambda is : {}'.format(sie_xishu))
    elif view > 1:  # ä»…ä½¿ç”¨è§†è§’åµŒå…¥
        self.sie_embed = nn.Parameter(torch.zeros(view, 1, embed_dim))
        trunc_normal_(self.sie_embed, std=.02)
        print('viewpoint number is : {}'.format(view))
        print('using SIE_Lambda is : {}'.format(sie_xishu))

    # æ‰“å°è®­ç»ƒé…ç½®ä¿¡æ¯
    print('using drop_out rate is : {}'.format(drop_rate))
    print('using attn_drop_out rate is : {}'.format(attn_drop_rate))
    print('using drop_path rate is : {}'.format(drop_path_rate))
    
    # åˆå§‹åŒ–Transformerå—åˆ—è¡¨
    dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # éšæœºæ·±åº¦è¡°å‡ç‡
    self.blocks = nn.ModuleList([
        Block(dim=embed_dim, num_heads=num_heads, mlp_ratio=mlp_ratio, qkv_bias=qkv_bias, 
              qk_scale=qk_scale, drop=drop_rate, attn_drop=attn_drop_rate, 
              drop_path=dpr[i], norm_layer=norm_layer)
        for i in range(depth)])
    self.norm = norm_layer(embed_dim)  # æœ€ç»ˆå±‚å½’ä¸€åŒ–

    # åˆ†ç±»å¤´
    self.head = nn.Linear(embed_dim, num_classes) if num_classes > 0 else nn.Identity()

    # æ–°å¢ï¼šå¤šå°ºåº¦æ»‘åŠ¨çª—å£æ¨¡å—
    self.use_multi_scale = use_multi_scale
    if self.use_multi_scale:
        from ..fusion_part.multi_scale_sliding_window import MultiScaleFeatureExtractor
        self.multi_scale_extractor = MultiScaleFeatureExtractor(embed_dim, scales=[4, 8, 16])
        print('å¯ç”¨å¤šå°ºåº¦æ»‘åŠ¨çª—å£ç‰¹å¾æå–æ¨¡å—')

    # åˆå§‹åŒ–æƒé‡
    trunc_normal_(self.cls_token, std=.02)
    self.apply(self._init_weights)
```

### ä¿®æ”¹ä½ç½®3ï¼šT2T_ViTç±»çš„forward_featuresæ–¹æ³•

**æ–‡ä»¶ä½ç½®**: `modeling/backbones/t2t.py`
**ä¿®æ”¹ä½ç½®**: `T2T_ViT.forward_features` æ–¹æ³•

```python
def forward_features(self, x, camera_id, view_id):
    """
    ç‰¹å¾æå–å‰å‘ä¼ æ’­ï¼ˆåŒ…å«SIEåµŒå…¥å’Œå¤šå°ºåº¦å¤„ç†ï¼‰
    
    å¤„ç†æµç¨‹ï¼š
    1. T2Tæ¨¡å—å°†å›¾åƒè½¬æ¢ä¸ºtokens
    2. æ·»åŠ åˆ†ç±»tokenå’Œä½ç½®ç¼–ç 
    3. é€šè¿‡Transformerå—è¿›è¡Œç‰¹å¾ç¼–ç 
    4. å¤šå°ºåº¦æ»‘åŠ¨çª—å£å¤„ç†ï¼ˆæ–°å¢ï¼‰
    5. è¿”å›å¢å¼ºåçš„ç‰¹å¾
    """
    B = x.shape[0]
    
    # é€šè¿‡T2Tæ¨¡å—å°†å›¾åƒè½¬æ¢ä¸ºtokens
    x = self.tokens_to_token(x)  # [B, N, embed_dim]

    # æ·»åŠ åˆ†ç±»token
    cls_tokens = self.cls_token.expand(B, -1, -1)  # [B, 1, embed_dim]
    x = torch.cat((cls_tokens, x), dim=1)  # [B, N+1, embed_dim]
    
    # æ·»åŠ ä½ç½®ç¼–ç å’ŒSIEåµŒå…¥
    if self.cam_num > 0 and self.view_num > 0:  # åŒæ—¶ä½¿ç”¨ç›¸æœºå’Œè§†è§’åµŒå…¥
        x = x + self.pos_embed + self.sie_xishu * self.sie_embed[camera_id * self.view_num + view_id]
    elif self.cam_num > 0:  # ä»…ä½¿ç”¨ç›¸æœºåµŒå…¥
        x = x + self.pos_embed + self.sie_xishu * self.sie_embed[camera_id]
    elif self.view_num > 0:  # ä»…ä½¿ç”¨è§†è§’åµŒå…¥
        x = x + self.pos_embed + self.sie_xishu * self.sie_embed[view_id]
    else:  # ä»…ä½¿ç”¨ä½ç½®ç¼–ç 
        x = x + self.pos_embed

    # åº”ç”¨dropout
    x = self.pos_drop(x)

    # é€šè¿‡å¤šå±‚Transformerå—è¿›è¡Œç‰¹å¾ç¼–ç 
    for blk in self.blocks:
        x = blk(x)

    # æœ€ç»ˆå±‚å½’ä¸€åŒ–
    x = self.norm(x)
    
    # æ–°å¢ï¼šå¤šå°ºåº¦æ»‘åŠ¨çª—å£å¤„ç†
    if self.use_multi_scale:
        # æå–CLS tokenå’Œpatch tokens
        cls_token = x[:, 0:1, :]  # [B, 1, embed_dim]
        patch_tokens = x[:, 1:, :]  # [B, N, embed_dim]
        
        # å¤šå°ºåº¦æ»‘åŠ¨çª—å£å¤„ç†patch tokens
        multi_scale_feature = self.multi_scale_extractor(patch_tokens)  # [B, embed_dim]
        
        # å°†å¤šå°ºåº¦ç‰¹å¾ä¸CLS tokenç»“åˆ
        enhanced_cls = cls_token + multi_scale_feature.unsqueeze(1)  # [B, 1, embed_dim]
        
        # é‡æ–°ç»„åˆtokens
        x = torch.cat([enhanced_cls, patch_tokens], dim=1)  # [B, N+1, embed_dim]
    
    return x
```

### ä¿®æ”¹ä½ç½®4ï¼št2t_vit_24æ¨¡å‹å·¥å‚å‡½æ•°

**æ–‡ä»¶ä½ç½®**: `modeling/backbones/t2t.py`
**ä¿®æ”¹ä½ç½®**: `t2t_vit_24` å‡½æ•°

```python
@register_model
def t2t_vit_24(img_size=(256, 128), stride_size=16, drop_path_rate=0.1, drop_rate=0.0,
               attn_drop_rate=0.0, camera=0, view=0, local_feature=False, sie_xishu=1.5, 
               pretrained=False, use_multi_scale=False, **kwargs):  # æ–°å¢å‚æ•°
    """
    T2T-ViT-24æ¨¡å‹å·¥å‚å‡½æ•°ï¼ˆæœ€å¤§è§„æ¨¡ç‰ˆæœ¬ï¼‰
    
    åŠŸèƒ½ï¼š
    - åˆ›å»ºæ·±åº¦ä¸º24å±‚çš„T2T-ViTæ¨¡å‹
    - ä½¿ç”¨Performerä½œä¸ºtokenç¼–ç å™¨
    - æœ€å¤§è§„æ¨¡é…ç½®ï¼Œæä¾›æœ€ä½³æ€§èƒ½
    - æ”¯æŒå¤šå°ºåº¦æ»‘åŠ¨çª—å£ç‰¹å¾æå–
    """
    if pretrained:
        kwargs.setdefault('qk_scale', 512 ** -0.5)
    model = T2T_ViT(img_size=img_size, drop_path_rate=drop_path_rate, drop_rate=drop_rate,
                    attn_drop_rate=attn_drop_rate, camera=camera, view=view, sie_xishu=sie_xishu,
                    tokens_type='performer', embed_dim=512, depth=24, num_heads=8, mlp_ratio=3., 
                    use_multi_scale=use_multi_scale, **kwargs)  # ä¼ é€’å‚æ•°
    model.default_cfg = default_cfgs['T2t_vit_24']
    if pretrained:
        load_pretrained(model, num_classes=model.num_classes, in_chans=kwargs.get('in_chans', 3))
    return model
```

### ä¿®æ”¹ä½ç½®5ï¼šbuild_transformerç±»çš„__init__æ–¹æ³•

**æ–‡ä»¶ä½ç½®**: `modeling/make_model.py`
**ä¿®æ”¹ä½ç½®**: `build_transformer.__init__` æ–¹æ³•

```python
class build_transformer(nn.Module):  # è§†è§‰éª¨å¹²å°è£…ï¼ˆå…¼å®¹ ViT/CLIP/T2T ç­‰ï¼‰
    def __init__(self, num_classes, cfg, camera_num, view_num, factory, feat_dim):
        super().__init__()
        model_path = cfg.MODEL.PRETRAIN_PATH_T  # é¢„è®­ç»ƒæƒé‡è·¯å¾„ï¼ˆImageNet/è‡ªå®šä¹‰ï¼‰
        self.in_planes = feat_dim  # ç‰¹å¾ç»´åº¦ï¼ˆçº¿æ€§åˆ†ç±»å™¨/BNNeckè¾“å…¥ï¼‰
        self.cv_embed_sign = cfg.MODEL.SIE_CAMERA  # æ˜¯å¦å¯ç”¨ç›¸æœº/è§†è§’åµŒå…¥
        self.neck = cfg.MODEL.NECK  # é¢ˆéƒ¨ç»“æ„ç±»å‹ï¼ˆå¦‚ bnneckï¼‰
        self.neck_feat = cfg.TEST.NECK_FEAT  # æµ‹è¯•é˜¶æ®µè¿”å› neck å‰/åç‰¹å¾
        self.model_name = cfg.MODEL.TRANSFORMER_TYPE  # éª¨å¹²ç±»å‹å
        self.trans_type = cfg.MODEL.TRANSFORMER_TYPE  # åŒä¸Š
        self.flops_test = cfg.MODEL.FLOPS_TEST  # FLOPs æµ‹è¯•æ ‡å¿—
        print('using Transformer_type: {} as a backbone'.format(cfg.MODEL.TRANSFORMER_TYPE))  # æ‰“å°éª¨å¹²ç±»å‹

        if cfg.MODEL.SIE_CAMERA:
            self.camera_num = camera_num  # ç›¸æœºæ•°é‡ï¼ˆç”¨äº SIEï¼‰
        else:
            self.camera_num = 0
        # No view
        self.view_num = 0  # è§†è§’æ•°æ­¤å¤„å›ºå®šä¸º0ï¼ˆå¦‚éœ€å¯æ‰©å±•ï¼‰
        
        # æ–°å¢ï¼šå¤šå°ºåº¦æ»‘åŠ¨çª—å£é…ç½®
        self.use_multi_scale = getattr(cfg.MODEL, 'USE_MULTI_SCALE', False)
        
        if cfg.MODEL.TRANSFORMER_TYPE == 'vit_base_patch16_224':
            self.base = factory[cfg.MODEL.TRANSFORMER_TYPE](
                img_size=cfg.INPUT.SIZE_TRAIN,
                stride_size=cfg.MODEL.STRIDE_SIZE,
                drop_path_rate=cfg.MODEL.DROP_PATH,
                drop_rate=cfg.MODEL.DROP_RATE,
                attn_drop_rate=cfg.MODEL.ATT_DROP_RATE,
                camera=camera_num,
                view=view_num,
                sie_xishu=cfg.MODEL.SIE_COE,
            )
        elif cfg.MODEL.TRANSFORMER_TYPE == 't2t_vit_t_24':
            self.base = factory[cfg.MODEL.TRANSFORMER_TYPE](
                img_size=cfg.INPUT.SIZE_TRAIN,
                stride_size=cfg.MODEL.STRIDE_SIZE,
                drop_path_rate=cfg.MODEL.DROP_PATH,
                drop_rate=cfg.MODEL.DROP_RATE,
                attn_drop_rate=cfg.MODEL.ATT_DROP_RATE,
                camera=camera_num,
                view=view_num,
                sie_xishu=cfg.MODEL.SIE_COE,
                use_multi_scale=self.use_multi_scale  # æ–°å¢å‚æ•°
            )
        # ... å…¶ä»–æ¨¡å‹ç±»å‹çš„å¤„ç† ...
        
        # BNNeck
        self.bottleneck.bias.requires_grad_(False)  # å†»ç»“åç½®
        self.bottleneck.apply(weights_init_kaiming)  # BN åˆå§‹åŒ–
```

### ä¿®æ”¹ä½ç½®6ï¼šé…ç½®æ–‡ä»¶

**æ–‡ä»¶ä½ç½®**: `configs/MSVR310/MambaPro.yml`
**ä¿®æ”¹ä½ç½®**: åœ¨MODELéƒ¨åˆ†æ·»åŠ é…ç½®

```yaml
MODEL:
  # ... ç°æœ‰é…ç½® ...
  USE_MULTI_SCALE: True  # å¯ç”¨å¤šå°ºåº¦æ»‘åŠ¨çª—å£
  MULTI_SCALE_SCALES: [4, 8, 16]  # æ»‘åŠ¨çª—å£å°ºåº¦
```

## ğŸ§ª æµ‹è¯•éªŒè¯

### æµ‹è¯•è„šæœ¬1ï¼šå¤šå°ºåº¦æ»‘åŠ¨çª—å£æ¨¡å—æµ‹è¯•

**æ–‡ä»¶ä½ç½®**: æ–°å»º `test_multi_scale_sliding_window.py`

```python
import torch
from modeling.fusion_part.multi_scale_sliding_window import MultiScaleSlidingWindow, MultiScaleFeatureExtractor

def test_multi_scale_sliding_window():
    """æµ‹è¯•å¤šå°ºåº¦æ»‘åŠ¨çª—å£æ¨¡å—"""
    print("å¼€å§‹æµ‹è¯•å¤šå°ºåº¦æ»‘åŠ¨çª—å£æ¨¡å—...")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size, seq_len, feat_dim = 2, 100, 512
    x = torch.randn(batch_size, seq_len, feat_dim)
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    
    # åˆ›å»ºå¤šå°ºåº¦æ»‘åŠ¨çª—å£æ¨¡å—
    multi_scale_window = MultiScaleSlidingWindow(feat_dim, scales=[4, 8, 16])
    
    # å‰å‘ä¼ æ’­
    output = multi_scale_window(x)
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
    
    # éªŒè¯è¾“å‡ºå½¢çŠ¶
    assert output.shape == (batch_size, feat_dim)
    print("âœ… å¤šå°ºåº¦æ»‘åŠ¨çª—å£æµ‹è¯•é€šè¿‡ï¼")

def test_multi_scale_feature_extractor():
    """æµ‹è¯•å¤šå°ºåº¦ç‰¹å¾æå–å™¨"""
    print("å¼€å§‹æµ‹è¯•å¤šå°ºåº¦ç‰¹å¾æå–å™¨...")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size, seq_len, feat_dim = 2, 100, 512
    x = torch.randn(batch_size, seq_len, feat_dim)
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    
    # åˆ›å»ºå¤šå°ºåº¦ç‰¹å¾æå–å™¨
    feature_extractor = MultiScaleFeatureExtractor(feat_dim, scales=[4, 8, 16])
    
    # å‰å‘ä¼ æ’­
    output = feature_extractor(x)
    print(f"è¾“å‡ºå½¢çŠ¶: {output.shape}")
    
    # éªŒè¯è¾“å‡ºå½¢çŠ¶
    assert output.shape == (batch_size, feat_dim)
    print("âœ… å¤šå°ºåº¦ç‰¹å¾æå–å™¨æµ‹è¯•é€šè¿‡ï¼")

if __name__ == "__main__":
    test_multi_scale_sliding_window()
    test_multi_scale_feature_extractor()
```

### æµ‹è¯•è„šæœ¬2ï¼šT2Tå¤šå°ºåº¦é›†æˆæµ‹è¯•

**æ–‡ä»¶ä½ç½®**: æ–°å»º `test_t2t_multi_scale.py`

```python
import torch
from modeling.backbones.t2t import t2t_vit_24

def test_t2t_with_multi_scale():
    """æµ‹è¯•é›†æˆå¤šå°ºåº¦æ»‘åŠ¨çª—å£çš„T2Tæ¨¡å‹"""
    print("å¼€å§‹æµ‹è¯•T2Tå¤šå°ºåº¦é›†æˆ...")
    
    # åˆ›å»ºæ¨¡å‹
    model = t2t_vit_24(use_multi_scale=True)
    print("âœ… æ¨¡å‹åˆ›å»ºæˆåŠŸ")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    x = torch.randn(2, 3, 256, 128)
    print(f"è¾“å…¥å½¢çŠ¶: {x.shape}")
    
    # å‰å‘ä¼ æ’­
    output = model(x)
    print(f"è¾“å‡ºå½¢çŠ¶: {len(output)} ä¸ªå…ƒç´ ")
    print(f"ç¬¬ä¸€ä¸ªå…ƒç´ å½¢çŠ¶: {output[0].shape}")
    
    print("âœ… T2Tå¤šå°ºåº¦é›†æˆæµ‹è¯•é€šè¿‡ï¼")

if __name__ == "__main__":
    test_t2t_with_multi_scale()
```

## ğŸ“Š é¢„æœŸæ•ˆæœ

### æ€§èƒ½æå‡
- **mAPæå‡**: é¢„æœŸæå‡1-2%
- **Rank-1æå‡**: é¢„æœŸæå‡1-2%
- **è®¡ç®—å¼€é”€**: å¢åŠ çº¦5-10%

### ç‰¹å¾è´¨é‡
- **å¤šå°ºåº¦æ„ŸçŸ¥**: åŒæ—¶æ•è·å±€éƒ¨ç»†èŠ‚å’Œå…¨å±€ä¸Šä¸‹æ–‡
- **æ»‘åŠ¨çª—å£æœºåˆ¶**: 4x4ã€8x8ã€16x16çš„å¤šå°ºåº¦çª—å£è®¾è®¡
- **ç‰¹å¾èåˆ**: å°†å¤šå°ºåº¦ç‰¹å¾æœ‰æ•ˆèåˆ

## ğŸ¯ ä¸å›¾ç‰‡æµç¨‹çš„å¯¹åº”å…³ç³»

### å›¾ç‰‡æµç¨‹ â†’ ä»£ç å®ç°

1. **"å¯¹æŸæ¨¡æ€å›¾åƒå¤„ç†å, å¾—åˆ° Tokenåºåˆ—"** 
   â†’ `T2T_module.forward()` è¾“å‡ºtokenåºåˆ—

2. **"Multi-Scaleå¤šå°ºåº¦ç‰¹å¾æå–"**
   â†’ `MultiScaleSlidingWindow` ç±»å®ç°

3. **"æ»‘åŠ¨çª—å£ Scale1/2/3"**
   â†’ `scales=[4, 8, 16]` å‚æ•°é…ç½®

4. **"å¾—åˆ°é•¿åºåˆ—"**
   â†’ æœ€ç»ˆè¾“å‡ºçš„å¢å¼ºç‰¹å¾åºåˆ—

## ğŸš¨ æ³¨æ„äº‹é¡¹

### 1. å†…å­˜ä½¿ç”¨
- å¤šå°ºåº¦æ»‘åŠ¨çª—å£ä¼šå¢åŠ å†…å­˜ä½¿ç”¨
- å»ºè®®åœ¨GPUå†…å­˜å……è¶³æ—¶ä½¿ç”¨

### 2. è®­ç»ƒç¨³å®šæ€§
- æ»‘åŠ¨çª—å£å°ºåº¦éœ€è¦æ ¹æ®æ•°æ®è°ƒæ•´
- å»ºè®®ä½¿ç”¨è¾ƒå°çš„å­¦ä¹ ç‡

### 3. è¶…å‚æ•°è°ƒä¼˜
- æ»‘åŠ¨çª—å£å°ºåº¦éœ€è¦æ ¹æ®æ•°æ®è°ƒæ•´
- ç‰¹å¾èåˆç½‘ç»œç»´åº¦å½±å“æ€§èƒ½

## ğŸ”„ å›æ»šæ–¹æ¡ˆ

å¦‚æœé›†æˆåå‡ºç°é—®é¢˜ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼å›æ»šï¼š

1. **é…ç½®å›æ»š**: è®¾ç½® `USE_MULTI_SCALE: False`
2. **ä»£ç å›æ»š**: åˆ é™¤æ–°å¢çš„å¤šå°ºåº¦å¤„ç†ä»£ç 
3. **æ¨¡å‹å›æ»š**: ä½¿ç”¨åŸå§‹æ¨¡å‹æƒé‡

## ğŸ“ æ€»ç»“

åŸºäºReidpic.pngå›¾ç‰‡çš„æµç¨‹åˆ†æï¼Œå¤šå°ºåº¦ç‰¹å¾æå–çš„é›†æˆéœ€è¦åœ¨ä»¥ä¸‹å…³é”®ä½ç½®è¿›è¡Œä¿®æ”¹ï¼š

1. **æ–°å»ºæ¨¡å—**: åˆ›å»ºå¤šå°ºåº¦æ»‘åŠ¨çª—å£æ¨¡å—
2. **T2T_ViTç±»**: åœ¨ç‰¹å¾æå–æµç¨‹ä¸­é›†æˆå¤šå°ºåº¦å¤„ç†
3. **é…ç½®æ–‡ä»¶**: æ·»åŠ å¤šå°ºåº¦æ»‘åŠ¨çª—å£ç›¸å…³é…ç½®
4. **æ¨¡å‹å·¥å‚**: æ›´æ–°æ¨¡å‹åˆ›å»ºé€»è¾‘
5. **æµ‹è¯•éªŒè¯**: ç¡®ä¿åŠŸèƒ½æ­£ç¡®æ€§å’Œæ€§èƒ½æå‡

**æ ¸å¿ƒåˆ›æ–°ç‚¹**ï¼š
- **å¤šå°ºåº¦æ»‘åŠ¨çª—å£**: 4x4ã€8x8ã€16x16çš„æ»‘åŠ¨çª—å£è®¾è®¡
- **ç‰¹å¾èåˆæœºåˆ¶**: å°†å¤šå°ºåº¦ç‰¹å¾æœ‰æ•ˆèåˆ
- **T2Té›†æˆ**: ä¸T2Tæœºåˆ¶æ— ç¼ç»“åˆ

é€šè¿‡è¿™ç§é›†æˆæ–¹å¼ï¼Œå¯ä»¥åœ¨ä¿æŒç°æœ‰æ¶æ„ç¨³å®šæ€§çš„åŒæ—¶ï¼Œå……åˆ†åˆ©ç”¨å¤šå°ºåº¦æ»‘åŠ¨çª—å£çš„åˆ›æ–°ä¼˜åŠ¿ï¼Œå®ç°ä»å±€éƒ¨ç»†èŠ‚åˆ°å…¨å±€ä¸Šä¸‹æ–‡çš„å…¨æ–¹ä½ç‰¹å¾æ•è·ã€‚